{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [\n",
    "    [\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
    "    [\"B-PER\", \"I-PER\", \"O\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Sequence\n",
    "\n",
    "\n",
    "class Matrics:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sents_true_labels: Sequence[Sequence[Dict]],\n",
    "        sents_pred_labels: Sequence[Sequence[Dict]],\n",
    "    ):\n",
    "        self.sents_true_labels = sents_true_labels\n",
    "        self.sents_pred_labels = sents_pred_labels\n",
    "        self.types = set(\n",
    "            entity[\"type\"] for sent in sents_true_labels for entity in sent\n",
    "        )\n",
    "        self.confusion_matrices = {\n",
    "            type: {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0} for type in self.types\n",
    "        }\n",
    "        self.scores = {type: {\"p\": 0, \"r\": 0, \"f1\": 0} for type in self.types}\n",
    "\n",
    "    def cal_confusion_matrices(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Calculate confusion matrices for all sentences.\"\"\"\n",
    "        for true_labels, pred_labels in zip(\n",
    "            self.sents_true_labels, self.sents_pred_labels\n",
    "        ):\n",
    "            for true_label in true_labels:\n",
    "                entity_type = true_label[\"type\"]\n",
    "                prediction_hit_count = 0\n",
    "                for pred_label in pred_labels:\n",
    "                    if pred_label[\"type\"] != entity_type:\n",
    "                        continue\n",
    "                    if (\n",
    "                        pred_label[\"start_idx\"] == true_label[\"start_idx\"]\n",
    "                        and pred_label[\"end_idx\"] == true_label[\"end_idx\"]\n",
    "                        and pred_label[\"text\"] == true_label[\"text\"]\n",
    "                    ):  # TP\n",
    "                        self.confusion_matrices[entity_type][\"TP\"] += 1\n",
    "                        prediction_hit_count += 1\n",
    "                    elif (\n",
    "                        (pred_label[\"start_idx\"] == true_label[\"start_idx\"])\n",
    "                        or (pred_label[\"end_idx\"] == true_label[\"end_idx\"])\n",
    "                    ) and pred_label[\"text\"] != true_label[\n",
    "                        \"text\"\n",
    "                    ]:  # boundry error, count FN, FP\n",
    "                        self.confusion_matrices[entity_type][\"FP\"] += 1\n",
    "                        self.confusion_matrices[entity_type][\"FN\"] += 1\n",
    "                        prediction_hit_count += 1\n",
    "                if (\n",
    "                    prediction_hit_count != 1\n",
    "                ):  # FN, model cannot make a prediction for true_label\n",
    "                    self.confusion_matrices[entity_type][\"FN\"] += 1\n",
    "                prediction_hit_count = 0  # reset to default\n",
    "\n",
    "    def cal_scores(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Calculate precision, recall, f1.\"\"\"\n",
    "        confusion_matrices = self.confusion_matrices\n",
    "        scores = {type: {\"p\": 0, \"r\": 0, \"f1\": 0} for type in self.types}\n",
    "\n",
    "        for entity_type, confusion_matrix in confusion_matrices.items():\n",
    "            if confusion_matrix[\"TP\"] == 0 and confusion_matrix[\"FP\"] == 0:\n",
    "                scores[entity_type][\"p\"] = 0\n",
    "            else:\n",
    "                scores[entity_type][\"p\"] = confusion_matrix[\"TP\"] / (\n",
    "                    confusion_matrix[\"TP\"] + confusion_matrix[\"FP\"]\n",
    "                )\n",
    "\n",
    "            if confusion_matrix[\"TP\"] == 0 and confusion_matrix[\"FN\"] == 0:\n",
    "                scores[entity_type][\"r\"] = 0\n",
    "            else:\n",
    "                scores[entity_type][\"r\"] = confusion_matrix[\"TP\"] / (\n",
    "                    confusion_matrix[\"TP\"] + confusion_matrix[\"FN\"]\n",
    "                )\n",
    "\n",
    "            if scores[entity_type][\"p\"] == 0 or scores[entity_type][\"r\"] == 0:\n",
    "                scores[entity_type][\"f1\"] = 0\n",
    "            else:\n",
    "                scores[entity_type][\"f1\"] = (\n",
    "                    2\n",
    "                    * scores[entity_type][\"p\"]\n",
    "                    * scores[entity_type][\"r\"]\n",
    "                    / (scores[entity_type][\"p\"] + scores[entity_type][\"r\"])\n",
    "                )\n",
    "        self.scores = scores\n",
    "\n",
    "    def print_confusion_matrices(self):\n",
    "        for entity_type, matrix in self.confusion_matrices.items():\n",
    "            print(f\"{entity_type}: {matrix}\")\n",
    "\n",
    "    def print_scores(self):\n",
    "        for entity_type, score in self.scores.items():\n",
    "            print(\n",
    "                f\"{entity_type}: f1 {score['f1']:.4f}, precision {score['p']:.4f}, recall {score['r']:.4f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sents_true_labels = [\n",
    "        [\n",
    "            {\"start_idx\": 0, \"end_idx\": 1, \"text\": \"Foreign Ministry\", \"type\": \"ORG\"},\n",
    "            {\"start_idx\": 3, \"end_idx\": 4, \"text\": \"Shen Guofang\", \"type\": \"PER\"},\n",
    "            {\"start_idx\": 6, \"end_idx\": 6, \"text\": \"Reuters\", \"type\": \"ORG\"},\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    sents_pred_labels = [\n",
    "        [\n",
    "            {\"start_idx\": 3, \"end_idx\": 3, \"text\": \"Shen\", \"type\": \"PER\"},\n",
    "            {\"start_idx\": 6, \"end_idx\": 6, \"text\": \"Reuters\", \"type\": \"ORG\"},\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    matrics = Matrics(sents_true_labels, sents_pred_labels)\n",
    "    matrics.cal_confusion_matrices()\n",
    "    matrics.print_confusion_matrices()\n",
    "    matrics.cal_scores()\n",
    "    matrics.print_scores()\n",
    "\n",
    "# PER: {'TP': 0, 'TN': 0, 'FP': 1, 'FN': 1}\n",
    "# ORG: {'TP': 1, 'TN': 0, 'FP': 0, 'FN': 1}\n",
    "# PER: f1 0.0000, precision 0.0000, recall 0.0000\n",
    "# ORG: f1 0.6667, precision 1.0000, recall 0.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "134d0c420be716f4705853afbda20cd302fe0206959a3d1297c95703aa0f5f9d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "Create a stratified (previously by taxons and document_type) sample of sentences to annotate, some of which are likely to contain instances of the following Entity Types:\n",
    "\n",
    "- FORM \n",
    "- LOCATION (PROPER NOUN / GPE)\n",
    "- ORG (PROPER NOUN)\n",
    "- PERSON (PROPER NOUN)\n",
    "- POSTCODE\n",
    "- EMAIL\n",
    "- PHONE N\n",
    "- DATE\n",
    "- MONEY Â£ (AMOUNT)\n",
    "\n",
    "We will include:\n",
    "\n",
    "- ALL the extracted contact details\n",
    "- 1000 titles with a ratio of 2.5:1 likley-to-contain-a-target-entity : likely-NOT-to-contain-a-target-entity  \n",
    "- 3000 sentences from body text with a ratio of 2.5:1 likley-to-contain-a-target-entity : likely-NOT-to-contain-a-target-entity  \n",
    "\n",
    "We will then create two samples to annotate to that it is easier to share the workload.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "A stratified random sample of pagepath's and their content, obtained from the MongoDB copy of the Content Store.\n",
    "Please see `src/make_strata/README.md`.\n",
    "\n",
    "This will ensure the input file `{SAMPLE_DATE}_stratified_sample_all_content.csv` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.make_data.make_data import text_col_to_sents\n",
    "from src.make_strata.sample_paths_by_strata import get_stratified_sample\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    disable=[\"tok2vec\", \"tagger\", \"parser\", \"lemmatizer\", \"attribute_ruler\"],\n",
    ")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# check what pipeline components will be applies\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = \"../../src/make_strata/data\"\n",
    "DIR_OUTPUT = os.environ.get(\"DIR_DATA_PROCESSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DATE = \"20220627\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUPUT_FILE1 = f\"{SAMPLE_DATE}_phase1_extra_training_examples_n{N}_p1.jsonl\"\n",
    "OUPUT_FILE2 = f\"{SAMPLE_DATE}_phase1_extra_training_examples_n{N}_p2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUPUT_FILEPATH1 = os.path.join(DIR_OUTPUT, OUPUT_FILE1)\n",
    "OUPUT_FILEPATH2 = os.path.join(DIR_OUTPUT, OUPUT_FILE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUPUT_FILEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = os.path.join(DIR_INPUT, f\"{SAMPLE_DATE}_stratified_sample_all_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Contact details\n",
    "\n",
    "We will add to the example set all the extracte contact details from `contact` and `organisation` pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_details_df = df.dropna(subset=[\"contact_details\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter relevant column\n",
    "contact_details_df = contact_details_df[\n",
    "    [\"base_path\", \"document_type\", \"schema_name\", \"contact_details\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text and process it to sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) From block of text to list of sentences as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing sentences...\")\n",
    "tic = time.perf_counter()\n",
    "ddf = dd.from_pandas(df, npartitions=os.cpu_count())\n",
    "res = ddf.map_partitions(lambda df: text_col_to_sents(df, \"text\"))\n",
    "out = res.compute()\n",
    "df = df.assign(sentences=out)\n",
    "toc = time.perf_counter()\n",
    "print(f\"Preprocessing of sentences - Completed in in {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"base_path\", \"document_type\", \"text\", \"sentences\"]][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Explode the dataframe by sentence, so that we have one sentence per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one sentence per row\n",
    "exploded_df = df[[\"base_path\", \"document_type\", \"text\", \"sentences\"]].explode(\n",
    "    \"sentences\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df[[\"base_path\", \"document_type\", \"text\", \"sentences\"]].sort_values(\n",
    "    by=[\"base_path\"]\n",
    ")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Convert `sentences` to spacy NLP objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude None rows\n",
    "exploded_df = exploded_df[~exploded_df.sentences.isnull()]\n",
    "exploded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentences = [nlp(sentence) for sentence in exploded_df.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Flag whether a sentence is likley to contain at least one of the \"target entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ENTITIES = set([\"DATE\", \"GPE\", \"LOC\", \"FAC\", \"MONEY\", \"ORG\", \"PERSON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contain possible target entity?\n",
    "contain_target_entity = []\n",
    "for doc in nlp_sentences:\n",
    "    if any(ent.label_ in TARGET_ENTITIES for ent in doc.ents):\n",
    "        contain_target_entity.append(True)\n",
    "    else:\n",
    "        contain_target_entity.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contain_target_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences that potentially contain one of the target entities\n",
    "sum(contain_target_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Merge flags  back to original dataframe for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df[\"contains_target_entities\"] = contain_target_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = df[[\"base_path\", \"document_type\", \"schema_name\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains possible Target entities?\n",
    "titles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_titles = [nlp(title) for title in titles_df.title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_contain_target_entity = []\n",
    "for doc in nlp_titles:\n",
    "    if any(ent.label_ in TARGET_ENTITIES for ent in doc.ents):\n",
    "        title_contain_target_entity.append(True)\n",
    "    else:\n",
    "        title_contain_target_entity.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(title_contain_target_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df[\"contains_target_entities\"] = title_contain_target_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random sampled stratified by document type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will include:\n",
    "- ALL the contact details\n",
    "- 1000 titles with a ratio of 2.5:1 likley-to-contain-a-target-entity : likely-NOT-to-contain-a-target-entity  \n",
    "- 3000 sentences from body text with a ratio of 2.5:1 likley-to-contain-a-target-entity : likely-NOT-to-contain-a-target-entity  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then create and export two samples to annotate to that it is easier to share the workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATA_WEIGHTS = {True: 2.5, False: 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_TITLES = 111\n",
    "rand_sample_titles = get_stratified_sample(\n",
    "    df=titles_df,\n",
    "    strata_col=\"contains_target_entities\",\n",
    "    weights=STRATA_WEIGHTS,\n",
    "    sample_size=600,\n",
    "    seed=SEED_TITLES,\n",
    ")\n",
    "rand_sample_titles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_TEXT = 222\n",
    "rand_sample_texts = get_stratified_sample(\n",
    "    df=exploded_df,\n",
    "    strata_col=\"contains_target_entities\",\n",
    "    weights=STRATA_WEIGHTS,\n",
    "    sample_size=2000,\n",
    "    seed=SEED_TEXT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample_texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contact details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_details_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged the random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure consistency of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample_texts = (\n",
    "    rand_sample_texts[[\"base_path\", \"document_type\", \"sentences\"]]\n",
    "    .rename(columns={\"sentences\": \"text\"})\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample_titles = (\n",
    "    rand_sample_titles[[\"base_path\", \"document_type\", \"title\"]]\n",
    "    .rename(columns={\"title\": \"text\"})\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_details = contact_details_df.rename(columns={\"contact_details\": \"text\"})[\n",
    "    [\"base_path\", \"document_type\", \"text\"]\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rand_sample = pd.concat(\n",
    "    [rand_sample_texts, rand_sample_titles, contact_details], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rand_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "total_rand_sample = total_rand_sample.sample(frac=1).copy()\n",
    "total_rand_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Prodigy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_df_to_jsonl(data: pd.DataFrame):\n",
    "    \"\"\"\"\"\"\n",
    "    collection = []\n",
    "    for base_path, doc_type, text in zip(\n",
    "        data[\"base_path\"], data[\"document_type\"], data[\"text\"]\n",
    "    ):\n",
    "        out_dict = {\n",
    "            \"text\": text,\n",
    "            \"meta\": {\"base_path\": base_path, \"doc_type\": doc_type},\n",
    "        }\n",
    "        collection.append(out_dict)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(a_list):\n",
    "    half = len(a_list) // 2\n",
    "    print(half)\n",
    "    return a_list[:half], a_list[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jsonl = from_df_to_jsonl(total_rand_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split it into 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample1, rand_sample2 = split_list(output_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to JSON lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_jsonl(file: List[dict], output_filpath: str):\n",
    "    with open(output_filpath, \"w\") as fp:\n",
    "        for item in file:\n",
    "            fp.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_jsonl(rand_sample1, OUPUT_FILEPATH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_jsonl(rand_sample2, OUPUT_FILEPATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

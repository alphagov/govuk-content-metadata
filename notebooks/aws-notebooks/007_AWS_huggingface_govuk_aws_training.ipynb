{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d304cfe1",
   "metadata": {},
   "source": [
    "# Test1: HuggingFace and AWS Sagemaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81767b0",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This is a notebook to test the HuggingFace transformers and datasets library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer for multi-class text classification.\n",
    "\n",
    "The pre-trained model will be fine-tuned using the govuk labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install datasets\n",
    "# !pip3 install transformers\n",
    "#!pip3 install sagemaker\n",
    "#!pip3 install torch\n",
    "#!pip3 install tensorflow\n",
    "#!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf398eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import secrets\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770ba36",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"AWS\"  # ['LOCAL', 'AWS']\n",
    "s3_bucket = \"govuk-data-infrastructure-integration\"\n",
    "s3_prefix = \"model-data/huggingface_transformer_models\"  # s3 key prefix for the data\n",
    "\n",
    "tokenizer_name = \"bert-base-uncased\"  # tokenizer used in preprocessing\n",
    "dataset_name = \"hf_data\"  # dataset used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if system == \"AWS\":\n",
    "    # set up sagemaker session offline to manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "    # sagemaker session bucket -> used for uploading data, models and logs. sagemaker will automatically create this bucket if it not exists\n",
    "    sess = sagemaker.Session()\n",
    "    sagemaker_session_bucket = s3_bucket\n",
    "    if sagemaker_session_bucket is None and sess is not None:\n",
    "        # set to default bucket if a bucket name is not given\n",
    "        sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "    print(f\"sagemaker role arn: {role}\")\n",
    "    print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "    print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n",
    "if system == \"LOCAL\":\n",
    "    s3 = boto3.resource(\n",
    "        service_name=\"s3\",\n",
    "        region_name=XXX,\n",
    "        aws_access_key_id=XXX,\n",
    "        aws_secret_access_key=XXX,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0be6a",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We are using the datasets library to download and preprocess the gov.uk labelled dataset. After preprocessing, the dataset will be uploaded to our sagemaker_session_bucket to be used within our training job. The gov.uk dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b601f",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29918b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer for 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "dataset_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/hf_data\"\n",
    "\n",
    "# load dataset\n",
    "ner_data = load_from_disk(dataset_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18bf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in dataset dict\n",
    "ner_data = ner_data.rename_column(\n",
    "    original_column_name=\"text_token\", new_column_name=\"tokens\"\n",
    ")\n",
    "ner_data = ner_data.rename_column(\n",
    "    original_column_name=\"new_label_list_id\", new_column_name=\"ner_tags\"\n",
    ")\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524715b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969eb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bf6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ea9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28909e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ner_feature.feature.id()\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_map = {\n",
    "    \"O\": 0,\n",
    "    \"CONTACT\": 1,\n",
    "    \"DATE\": 2,\n",
    "    \"EVENT\": 3,\n",
    "    \"FINANCE\": 4,\n",
    "    \"FORM\": 5,\n",
    "    \"LOCATION\": 6,\n",
    "    \"MISC\": 7,\n",
    "    \"MONEY\": 8,\n",
    "    \"ORGANIZATION\": 9,\n",
    "    \"PERSON\": 10,\n",
    "    \"SCHEME\": 11,\n",
    "    \"STATE\": 12,\n",
    "}\n",
    "\n",
    "label_names = [i for i in new_label_map.keys()]\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd832357",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ner_data[\"train\"][2][\"tokens\"]\n",
    "labels = ner_data[\"train\"][2][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04166a61",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62870886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c92337",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(ner_data[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367513be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75690b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ner_data[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(len(labels))\n",
    "print(labels)\n",
    "print(len(word_ids))\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca64d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e54481",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ner_data = ner_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=ner_data[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a120143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_ner_data[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8625ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9228d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce72933",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ner_data[\"train\"],\n",
    "    eval_dataset=tokenized_ner_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5c2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726851ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9710b408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2d705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec38aa5",
   "metadata": {},
   "source": [
    "### Uploading data to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets we are going to use the new FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d7905",
   "metadata": {},
   "source": [
    "## Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as entry_point, which instance_type should be used, which hyperparameters are passed in ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a3283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize ./scripts/ner_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python ./scripts/ner_training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f470574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"epochs\": 1,  # number of training epochs\n",
    "    \"train_batch_size\": 32,  # batch size for training\n",
    "    \"eval_batch_size\": 64,  # batch size for evaluation\n",
    "    \"learning_rate\": 3e-5,  # learning rate used during training\n",
    "    \"model_id\": \"distilbert-base-uncased\",  # pre-trained model\n",
    "    \"fp16\": True,  # Whether to use 16-bit (mixed) precision training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96173e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name\n",
    "job_name = f'huggingface-test-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",  # fine-tuning script used in training jon\n",
    "    source_dir=\"./scripts\",  # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3.2xlarge\",  #   # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version=\"4.6.1\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"1.7.1\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py36\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3addb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"train\": training_input_path, \"test\": test_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef99d9",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call deploy() on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82438e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf476c36",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cbbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [{\"inputs\": \"I get so nervous before a demo\"}, #fear\n",
    "#              {\"inputs\": \"I am shocked that the API works so well \"}, #suprise\n",
    "#              {\"inputs\": \"It's a shame that I havent learned this sooner\"}, #sadness\n",
    "#              {\"inputs\": \"It's a disgrace that AWS is not free\"}, #anger\n",
    "#              {\"inputs\": \"I am delighted to have learned this amazing new technology\"}, #joy\n",
    "#              {\"inputs\": \"I was so shocked at my suprise party. I also hated every minute of it.\"} #suprise/anger\n",
    "#             ]\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     prediction = predictor.predict(sentence)\n",
    "#     print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f12eb8",
   "metadata": {},
   "source": [
    "**IMPORTANT** Finally, we delete the inference endpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea030de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

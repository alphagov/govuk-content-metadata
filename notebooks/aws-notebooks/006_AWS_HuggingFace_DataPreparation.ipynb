{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c357d4",
   "metadata": {},
   "source": [
    "# HuggingFace Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5f73a",
   "metadata": {},
   "source": [
    "This is a notebook to prepare the labelled token dataset for HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22046350",
   "metadata": {},
   "source": [
    "## 1. Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from ast import literal_eval\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import transformers\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21239bc3",
   "metadata": {},
   "source": [
    "## 2. Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"AWS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc54bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if system == \"AWS\":\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_bucket = \"govuk-data-infrastructure-integration\"\n",
    "    DATA_DIR = f\"s3://{s3_bucket}/model-data/govner-data\"\n",
    "    for f in fs.ls(DATA_DIR):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = s3_bucket\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa278f",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 key prefix for the data\n",
    "s3_prefix = \"model-data/govner-data\"\n",
    "\n",
    "dataset1_name = \"line_by_line_NER_data_sampled_12062020_more_ents.csv\"\n",
    "dataset2_name = \"line_by_line_NER_data_sampled_09062020_more_ents.csv\"\n",
    "\n",
    "dataset1_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/{dataset1_name}\"\n",
    "dataset2_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/{dataset2_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a696b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv(dataset1_path, sep=\"\\t\", low_memory=False)\n",
    "dataset2 = pd.read_csv(dataset2_path, sep=\"\\t\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57348f",
   "metadata": {},
   "source": [
    "## 4. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e669a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset1 shape: {dataset1.shape}\")\n",
    "print(f\"dataset2 shape: {dataset2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da7f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e36a9",
   "metadata": {},
   "source": [
    "## 5. Concatenation\n",
    "\n",
    "We will concatenate the DaataFrames. They are likely separate for storage/memory reasons. We will combine and shuffle them anyway. We will also add a flag to show what dataset they were originally from too, for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0134c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1[\"original_file\"] = \"line_by_line_NER_data_sampled_12062020_more_ents.csv\"\n",
    "dataset2[\"original_file\"] = \"line_by_line_NER_data_sampled_09062020_more_ents.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be4ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a49bfa",
   "metadata": {},
   "source": [
    "Combine into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc15dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [dataset1, dataset2]\n",
    "concat = pd.concat(frames)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b60c48",
   "metadata": {},
   "source": [
    "Shuffle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = concat.sample(frac=1).reset_index(drop=True)\n",
    "print(shuffled_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb292386",
   "metadata": {},
   "source": [
    "Convert string list columns to list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1080ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df[\"text_token\"] = shuffled_df[\"text_token\"].apply(lambda x: literal_eval(x))\n",
    "shuffled_df[\"labels\"] = shuffled_df[\"labels\"].apply(lambda x: literal_eval(x))\n",
    "shuffled_df[\"label_list\"] = shuffled_df[\"label_list\"].apply(lambda x: literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa36b13",
   "metadata": {},
   "source": [
    "## 6. Label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map1_name = \"label_map_12062020_more_ents.json\"\n",
    "label_map2_name = \"label_map_09062020_more_ents.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7527863",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map1_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/{label_map1_name}\"\n",
    "label_map2_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/{label_map2_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if system == \"AWS\":\n",
    "    with fs.open(label_map1_path, \"rb\") as f:\n",
    "        label_name_map = json.load(f)\n",
    "    print(label_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7397c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if system == \"AWS\":\n",
    "    with fs.open(label_map2_path, \"rb\") as f:\n",
    "        label_name_map = json.load(f)\n",
    "    print(label_name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f5076",
   "metadata": {},
   "source": [
    "Alter label map.\n",
    "\n",
    "Why:\n",
    "* We dont need a label for 'PAD' that will be added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_map = {\n",
    "    \"O\": 0,\n",
    "    \"CONTACT\": 1,\n",
    "    \"DATE\": 2,\n",
    "    \"EVENT\": 3,\n",
    "    \"FINANCE\": 4,\n",
    "    \"FORM\": 5,\n",
    "    \"LOCATION\": 6,\n",
    "    \"MISC\": 7,\n",
    "    \"MONEY\": 8,\n",
    "    \"ORGANIZATION\": 9,\n",
    "    \"PERSON\": 10,\n",
    "    \"SCHEME\": 11,\n",
    "    \"STATE\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150cc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e86e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = shuffled_df[\"label_list\"][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_list_id(labellist, dictionary):\n",
    "    return [dictionary[x] for x in labellist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list_id(labellist=test, dictionary=new_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0def9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df[\"new_label_list_id\"] = shuffled_df[\"label_list\"].apply(\n",
    "    lambda x: label_list_id(x, new_label_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332d37c",
   "metadata": {},
   "source": [
    "Trim DataFrame to only the useful columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_df = shuffled_df[[\"text_token\", \"new_label_list_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19546f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(hf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518667b7",
   "metadata": {},
   "source": [
    "## 8. Train/Eval/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc78f36",
   "metadata": {},
   "source": [
    "We must split the data into Training, Evaluation and Test splits.\n",
    "\n",
    "CONLL Dataset Has the following spits:\n",
    "* Training: 14,041\n",
    "* Evaluation: 3,250\n",
    "* Test: 3,454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a71cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_training = {\"name\": \"training\", \"total\": 14041}\n",
    "conll_evaluation = {\"name\": \"evaluation\", \"total\": 3250}\n",
    "conll_test = {\"name\": \"test\", \"total\": 3454}\n",
    "\n",
    "total = conll_training[\"total\"] + conll_evaluation[\"total\"] + conll_test[\"total\"]\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [conll_training, conll_evaluation, conll_test]:\n",
    "    i[\"proportion\"] = (i[\"total\"] / total) * 100\n",
    "    print(i[\"name\"], i[\"proportion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d649fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = hf_dataset.train_test_split(train_size=0.85, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb97c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_clean = hf_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "hf_dataset_clean[\"validation\"] = hf_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "hf_dataset_clean[\"test\"] = hf_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7908f",
   "metadata": {},
   "source": [
    "## 9. Upload splits to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets we are going to use the new FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39abb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = \"model-data/huggingface_transformer_models/hf_data/\"\n",
    "print(data_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ff521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "hf_input_path = f\"s3://{sess.default_bucket()}/{data_prefix}\"\n",
    "hf_dataset_clean.save_to_disk(hf_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f315e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

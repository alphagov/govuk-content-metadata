{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33dfffad",
   "metadata": {},
   "source": [
    "# Test1: HuggingFace and AWS Sagemaker Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86ca11",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This is a notebook to test the HuggingFace transformers and datasets library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer for multi-class text classification.\n",
    "\n",
    "The pre-trained model will be fine-tuned using the emotion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7390672",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20827d6",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafee57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"govuk-data-infrastructure-integration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ec631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = (\n",
    "    sagemaker.Session()\n",
    ")  # Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = s3_bucket\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2199668",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We are using the datasets library to download and preprocess the emotion dataset. After preprocessing, the dataset will be uploaded to our sagemaker_session_bucket to be used within our training job. The emotion dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb68e9",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# dataset used\n",
    "dataset_name = \"emotion\"\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = \"model-data/huggingface_transformer_models/test_bucket/emotion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea28068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer for 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=[\"train\", \"test\"])\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958582f9",
   "metadata": {},
   "source": [
    "### Uploading data to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets we are going to use the new FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/train\"\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f\"s3://{sess.default_bucket()}/{s3_prefix}/test\"\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61accddd",
   "metadata": {},
   "source": [
    "## Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an HuggingFace Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as entry_point, which instance_type should be used, which hyperparameters are passed in ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d297bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"epochs\": 1,  # number of training epochs\n",
    "    \"train_batch_size\": 32,  # batch size for training\n",
    "    \"eval_batch_size\": 64,  # batch size for evaluation\n",
    "    \"learning_rate\": 3e-5,  # learning rate used during training\n",
    "    \"model_id\": \"distilbert-base-uncased\",  # pre-trained model\n",
    "    \"fp16\": True,  # Whether to use 16-bit (mixed) precision training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name\n",
    "job_name = f'huggingface-test-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",  # fine-tuning script used in training jon\n",
    "    source_dir=\"./scripts\",  # directory where fine-tuning script is stored\n",
    "    instance_type=\"ml.p3.2xlarge\",  #   # instances type used for the training job\n",
    "    instance_count=1,  # the number of instances used for training\n",
    "    base_job_name=job_name,  # the name of the training job\n",
    "    role=role,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version=\"4.6.1\",  # the transformers version used in the training job\n",
    "    pytorch_version=\"1.7.1\",  # the pytorch_version version used in the training job\n",
    "    py_version=\"py36\",  # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,  # the hyperparameter used for running the training job\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b31552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"train\": training_input_path, \"test\": test_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9924acaf",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call deploy() on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea311f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d53fb",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [{\"inputs\": \"I get so nervous before a demo\"}, #fear\n",
    "#              {\"inputs\": \"I am shocked that the API works so well \"}, #suprise\n",
    "#              {\"inputs\": \"It's a shame that I havent learned this sooner\"}, #sadness\n",
    "#              {\"inputs\": \"It's a disgrace that AWS is not free\"}, #anger\n",
    "#              {\"inputs\": \"I am delighted to have learned this amazing new technology\"}, #joy\n",
    "#              {\"inputs\": \"I was so shocked at my suprise party. I also hated every minute of it.\"} #suprise/anger\n",
    "#             ]\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     prediction = predictor.predict(sentence)\n",
    "#     print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d49986",
   "metadata": {},
   "source": [
    "**IMPORTANT** Finally, we delete the inference endpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

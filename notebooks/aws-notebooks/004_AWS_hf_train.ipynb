{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval\n",
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59935927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers[sentencepiece]\n",
    "# !apt install git-lfs\n",
    "\n",
    "# !git config --global user.email \"you@example.com\"\n",
    "# !git config --global user.name \"Your Name\"\n",
    "\n",
    "import numpy as np\n",
    "import seqeval\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "def main():\n",
    "    def check_cuda():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.current_device()\n",
    "            print(torch.cuda.current_device())\n",
    "            torch.cuda.device(0)\n",
    "            print(torch.cuda.device(0))\n",
    "            torch.cuda.device_count()\n",
    "            print(torch.cuda.device_count())\n",
    "            torch.cuda.get_device_name(0)\n",
    "            print(torch.cuda.get_device_name(0))\n",
    "        else:\n",
    "            print(\"No GPU Available.\")\n",
    "\n",
    "    check_cuda()\n",
    "\n",
    "    # load raw dataset\n",
    "    raw_datasets = load_dataset(\"conll2003\")\n",
    "\n",
    "    # get features from ner tags\n",
    "    ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "    # get feature label names\n",
    "    label_names = ner_feature.feature.names\n",
    "\n",
    "    # load bert model\n",
    "    model_checkpoint = \"bert-base-cased\"\n",
    "    # load tokeniser\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # align labels with tokens\n",
    "    def align_labels_with_tokens(labels, word_ids):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # Start of a new word!\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else labels[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id is None:\n",
    "                # Special token\n",
    "                new_labels.append(-100)\n",
    "            else:\n",
    "                # Same word as previous token\n",
    "                label = labels[word_id]\n",
    "                # If the label is B-XXX we change it to I-XXX\n",
    "                if label % 2 == 1:\n",
    "                    label += 1\n",
    "                new_labels.append(label)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "    # tokenise and align labels\n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "        )\n",
    "        all_labels = examples[\"ner_tags\"]\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(all_labels):\n",
    "            word_ids = tokenized_inputs.word_ids(i)\n",
    "            new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = new_labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    # tokenise datasets\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    )\n",
    "\n",
    "    # data collator\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    # metrics\n",
    "    metric = load_metric(\"seqeval\")\n",
    "\n",
    "    # compute metrics\n",
    "    def compute_metrics(eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "        # Remove ignored index (special tokens) and convert to labels\n",
    "        true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [\n",
    "            [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        all_metrics = metric.compute(\n",
    "            predictions=true_predictions, references=true_labels\n",
    "        )\n",
    "        return {\n",
    "            \"precision\": all_metrics[\"overall_precision\"],\n",
    "            \"recall\": all_metrics[\"overall_recall\"],\n",
    "            \"f1\": all_metrics[\"overall_f1\"],\n",
    "            \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "        }\n",
    "\n",
    "    # label id mappings\n",
    "    id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    # define model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    # define training arguments\n",
    "    args = TrainingArguments(\n",
    "        \"bert-finetuned-ner\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        push_to_hub=False,\n",
    "        hub_token=XXX,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from datasets import inspect_metric\n",
    "from seqeval.metrics import accuracy_score, classification_report, f1_score\n",
    "from seqeval.scheme import IOB1, IOB2, IOBES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"A\", \"B\", \"B\", \"A\", \"C\"]]\n",
    "y_pred = [[\"A\", \"B\", \"B\", \"A\", \"C\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(y_true, y_pred)))\n",
    "print(\"Acc Score: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "print(\"CR Report: {}\".format(classification_report(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"A\", \"B\", \"B\", \"A\", \"C\"], [\"A\", \"B\", \"C\"]]\n",
    "y_pred = [[\"A\", \"B\", \"B\", \"A\", \"C\"], [\"A\", \"B\", \"B\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(y_true, y_pred)))\n",
    "print(\"Acc Score: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "print(\"CR Report: {}\".format(classification_report(y_true, y_pred, digits=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"O\", \"B\", \"B\", \"I\", \"E\"], [\"O\", \"B\", \"E\"]]\n",
    "y_pred = [[\"O\", \"B\", \"B\", \"I\", \"E\"], [\"O\", \"B\", \"B\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(y_true, y_pred)))\n",
    "print(\"Acc Score: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "print(\"CR Report: {}\".format(classification_report(y_true, y_pred, digits=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"I-A\", \"I-B\", \"B\", \"I-A\", \"C\"], [\"I-A\", \"I-B\", \"C\"]]\n",
    "y_pred = [[\"I-A\", \"I-B\", \"B\", \"I-A\", \"C\"], [\"I-A\", \"I-B\", \"I-B\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(y_true, y_pred)))\n",
    "print(\"Acc Score: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "print(\"CR Report: {}\".format(classification_report(y_true, y_pred, digits=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\n",
    "    [\"I-AGRICULTURAL\", \"I-BUSINESS\", \"I-BUSINESS\", \"I-AGRICULTURAL\", \"C\"],\n",
    "    [\"I-AGRICULTURAL\", \"B-BUSINESS\", \"C\"],\n",
    "]\n",
    "y_pred = [\n",
    "    [\"I-AGRICULTURAL\", \"I-BUSINESS\", \"B-BUSINESS\", \"I-AGRICULTURAL\", \"C\"],\n",
    "    [\"I-AGRICULTURAL\", \"B-BUSINESS\", \"B-BUSINESS\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score: {}\".format(f1_score(y_true, y_pred)))\n",
    "print(\"Acc Score: {}\".format(accuracy_score(y_true, y_pred)))\n",
    "print(\"CR Report: \\n{}\".format(classification_report(y_true, y_pred, digits=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 - Custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(llist):\n",
    "    prev_tag = \"\"\n",
    "    indices = []\n",
    "    for i, ent in enumerate(llist):\n",
    "        if ent != prev_tag:\n",
    "            indices.append([ent, i, i])\n",
    "        else:\n",
    "            indices[-1][2] = i\n",
    "        prev_tag = ent\n",
    "    return [tuple(i) for i in indices if i[0] != \"O\"]\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    # intersection of predicted and true indexed named\n",
    "    # entities\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "\n",
    "    return nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    return nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "\n",
    "def classification_report(y_true, y_pred, digits=2):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "    last_line_heading = \"macro avg\"\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = \"{:>{width}s} \" + \" {:>9}\" * len(headers)\n",
    "    report = head_fmt.format(\"\", *headers, width=width)\n",
    "    report += \"\\n\\n\"\n",
    "\n",
    "    row_fmt = \"{:>{width}s} \" + \" {:>9.{digits}f}\" * 3 + \" {:>9}\\n\"\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, true_entities in d1.items():\n",
    "        pred_entities = d2[type_name]\n",
    "        nb_correct = len(true_entities & pred_entities)\n",
    "        nb_pred = len(pred_entities)\n",
    "        nb_true = len(true_entities)\n",
    "\n",
    "        p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(\n",
    "            *[type_name, p, r, f1, nb_true], width=width, digits=digits\n",
    "        )\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += \"\\n\"\n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format(\n",
    "        \"micro avg\",\n",
    "        precision_score(y_true, y_pred),\n",
    "        recall_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred),\n",
    "        np.sum(s),\n",
    "        width=width,\n",
    "        digits=digits,\n",
    "    )\n",
    "    report += row_fmt.format(\n",
    "        last_line_heading,\n",
    "        np.average(ps, weights=s),\n",
    "        np.average(rs, weights=s),\n",
    "        np.average(f1s, weights=s),\n",
    "        np.sum(s),\n",
    "        width=width,\n",
    "        digits=digits,\n",
    "    )\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "y_true = [\"a\", \"a\", \"b\", \"o\", \"o\", \"i\", \"a\"]\n",
    "y_pred = [\"a\", \"a\", \"O\", \"o\"]\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect 'seqeval' metric"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

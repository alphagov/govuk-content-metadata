{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import accuracy_score, classification_report, f1_score\n",
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\n",
    "    [\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"EVENT\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"EVENT\",\n",
    "        \"HELLO\",\n",
    "        \"TESTING\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "    ],\n",
    "    [\"B-PER\", \"I-PER\", \"O\"],\n",
    "]\n",
    "y_pred = [\n",
    "    [\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"EVENT\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "        \"HELLO\",\n",
    "        \"HELLO\",\n",
    "        \"HELLO\",\n",
    "        \"O\",\n",
    "        \"O\",\n",
    "    ],\n",
    "    [\"B-PER\", \"I-PER\", \"O\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, scheme=IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(llist):\n",
    "    prev_tag = \"\"\n",
    "    indices = []\n",
    "    for i, ent in enumerate(llist):\n",
    "        if ent != prev_tag:\n",
    "            indices.append([ent, i, i])\n",
    "        else:\n",
    "            indices[-1][2] = i\n",
    "        prev_tag = ent\n",
    "    return [tuple(i) for i in indices if i[0] != \"O\"]\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    # intersection of predicted and true indexed named\n",
    "    # entities\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "    return 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "\n",
    "    return nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    return nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "\n",
    "def classification_report(y_true, y_pred, digits=2):\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "    last_line_heading = \"macro avg\"\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u\"{:>{width}s} \" + u\" {:>9}\" * len(headers)\n",
    "    report = head_fmt.format(u\"\", *headers, width=width)\n",
    "    report += u\"\\n\\n\"\n",
    "\n",
    "    row_fmt = u\"{:>{width}s} \" + u\" {:>9.{digits}f}\" * 3 + u\" {:>9}\\n\"\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, true_entities in d1.items():\n",
    "        pred_entities = d2[type_name]\n",
    "        nb_correct = len(true_entities & pred_entities)\n",
    "        nb_pred = len(pred_entities)\n",
    "        nb_true = len(true_entities)\n",
    "\n",
    "        p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(\n",
    "            *[type_name, p, r, f1, nb_true], width=width, digits=digits\n",
    "        )\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += u\"\\n\"\n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format(\n",
    "        \"micro avg\",\n",
    "        precision_score(y_true, y_pred),\n",
    "        recall_score(y_true, y_pred),\n",
    "        f1_score(y_true, y_pred),\n",
    "        np.sum(s),\n",
    "        width=width,\n",
    "        digits=digits,\n",
    "    )\n",
    "    report += row_fmt.format(\n",
    "        last_line_heading,\n",
    "        np.average(ps, weights=s),\n",
    "        np.average(rs, weights=s),\n",
    "        np.average(f1s, weights=s),\n",
    "        np.sum(s),\n",
    "        width=width,\n",
    "        digits=digits,\n",
    "    )\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"EVENT\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"EVENT\",\n",
    "    \"HELLO\",\n",
    "    \"TESTING\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "]\n",
    "y_pred = [\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"EVENT\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "    \"EVENT\",\n",
    "    \"TESTING\",\n",
    "    \"TESTING\",\n",
    "    \"O\",\n",
    "    \"O\",\n",
    "]\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

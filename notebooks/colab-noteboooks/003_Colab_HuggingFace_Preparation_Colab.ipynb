{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c357d4",
   "metadata": {},
   "source": [
    "# HuggingFace Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a5f73a",
   "metadata": {},
   "source": [
    "This is a notebook to prepare the labelled token dataset for HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22046350",
   "metadata": {},
   "source": [
    "## 1. Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install s3fs\n",
    "!pip install boto3\n",
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from ast import literal_eval\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import transformers\n",
    "from datasets import ClassLabel, Dataset, Sequence, load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21239bc3",
   "metadata": {},
   "source": [
    "## 2. Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"COLAB\"  # [\"AWS\", \"COLAB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc54bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if system == \"AWS\":\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    s3_bucket = \"govuk-data-infrastructure-integration\"\n",
    "    DATA_DIR = f\"s3://{s3_bucket}/model-data/govner-data\"\n",
    "    for f in fs.ls(DATA_DIR):\n",
    "        print(f)\n",
    "    # Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "    # sagemaker session bucket -> used for uploading data, models and logs\n",
    "    # sagemaker will automatically create this bucket if it not exists\n",
    "    sess = sagemaker.Session()\n",
    "    sagemaker_session_bucket = s3_bucket\n",
    "    if sagemaker_session_bucket is None and sess is not None:\n",
    "        # set to default bucket if a bucket name is not given\n",
    "        sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "    print(f\"sagemaker role arn: {role}\")\n",
    "    print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "    print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "elif system == \"COLAB\":\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    # DATA_DIR = os.path.join(\"/content/gdrive/My Drive\", \"NER/Data\")\n",
    "    DATA_DIR = os.path.join(\n",
    "        \"/content/gdrive/Shareddrives/\",\n",
    "        \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UNS04nievzrX",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa278f",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QVCSAh6huaGz",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"govuk-labelled-data-ner.csv\"\n",
    "\n",
    "file_path = f\"{DATA_DIR}/{file_name}\"\n",
    "\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ze-Pl_nP_XMA",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_name = \"new_label_map.json\"\n",
    "\n",
    "label_map_path = f\"{DATA_DIR}/{label_map_name}\"\n",
    "\n",
    "print(label_map_path)\n",
    "\n",
    "with open(label_map_path) as f:\n",
    "    label_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HTFYbZBNBA0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S9hg9PpIuVKF",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xucHkEuhuwCL",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bgCXr5n0ZY2",
   "metadata": {},
   "source": [
    "Evaluate literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "II9t7T2V0Eks",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"text_token\", \"label_list\", \"new_label_list_id\"]:\n",
    "    print(col)\n",
    "    df[col] = df[col].map(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332d37c",
   "metadata": {},
   "source": [
    "Trim DataFrame to only the useful columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trim = df[[\"text_token\", \"new_label_list_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19546f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(df_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DfSxYz-OwU8a",
   "metadata": {},
   "source": [
    "## 4. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sscZmaxCwtDQ",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_dataset[\"text_token\"][9])\n",
    "print(hf_dataset[\"new_label_list_id\"][9])\n",
    "# print(hf_dataset['new_label_list_id'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DrdxPfEirCbs",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [\"text_token\", \"new_label_list_id\"]:\n",
    "    print(\"{}: {}\".format(j, hf_dataset.features[f\"{j}\"]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nAB1NnDqAcfm",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i for i in label_map.keys()]\n",
    "print(len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16pyk6gr30Et",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset.features[\"new_label_list_id\"] = Sequence(ClassLabel(13, labels), -1, id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qdqu65xKDzCD",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset.features[\"new_label_list_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n_TPVN09EX2-",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = hf_dataset.features[\"new_label_list_id\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518667b7",
   "metadata": {},
   "source": [
    "## 8. Train/Eval/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc78f36",
   "metadata": {},
   "source": [
    "We must split the data into Training, Evaluation and Test splits.\n",
    "\n",
    "CONLL Dataset Has the following spits:\n",
    "* Training: 14,041\n",
    "* Evaluation: 3,250\n",
    "* Test: 3,454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a71cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_training = {\"name\": \"training\", \"total\": 14041}\n",
    "conll_evaluation = {\"name\": \"evaluation\", \"total\": 3250}\n",
    "conll_test = {\"name\": \"test\", \"total\": 3454}\n",
    "\n",
    "total = conll_training[\"total\"] + conll_evaluation[\"total\"] + conll_test[\"total\"]\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [conll_training, conll_evaluation, conll_test]:\n",
    "    i[\"proportion\"] = (i[\"total\"] / total) * 100\n",
    "    print(i[\"name\"], i[\"proportion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HjKdB_oBOF2z",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d649fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = hf_dataset.train_test_split(train_size=0.85, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb97c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xUYcaDRzu1Uq",
   "metadata": {},
   "source": [
    "Add validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_dataset_clean = hf_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# # Rename the default \"test\" split to \"validation\"\n",
    "# hf_dataset_clean[\"validation\"] = hf_dataset_clean.pop(\"test\")\n",
    "# # Add the \"test\" set to our `DatasetDict`\n",
    "# hf_dataset_clean[\"test\"] = hf_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7908f",
   "metadata": {},
   "source": [
    "## 9. Upload splits to gdrive\n",
    "\n",
    "After we processed the datasets we are going to upload our dataset to gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hy6eGH0_DGns",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"hf_govuk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7xFbTn6RC_HY",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_path = f\"{DATA_DIR}/{dataset_name}\"\n",
    "dataset_name_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ff521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to gdrive\n",
    "hf_input_path = f\"{dataset_name_path}\"\n",
    "hf_dataset.save_to_disk(hf_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TPOZPOEDKRc2",
   "metadata": {},
   "source": [
    "## 10. Download Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MRD6v5TyKWpK",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "\n",
    "hf_data = \"hf_govuk_data\"\n",
    "\n",
    "hf_data_path = f\"{DATA_DIR}/{hf_data}\"\n",
    "hf_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ItUoS3boKZCb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk(hf_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TvQswrhXvW25",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TbqtXJKnvfrQ",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1QW4M_W1Vi-",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# scraping\n",
    "import requests\n",
    "import spacy\n",
    "\n",
    "# inference\n",
    "import torch\n",
    "import transformers\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import ClassLabel, Sequence, load_dataset, load_from_disk, load_metric\n",
    "from google.colab import drive\n",
    "from IPython.display import HTML, display\n",
    "from seqeval.metrics import accuracy_score\n",
    "from spacy import displacy\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"COLAB\"  # [\"AWS\", \"COLAB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if system == \"COLAB\":\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    DATA_DIR = os.path.join(\n",
    "        \"/content/gdrive/Shared drives/\",\n",
    "        \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data\",\n",
    "    )\n",
    "    MODEL_DIR = os.path.join(\n",
    "        \"/content/gdrive/Shared drives/\",\n",
    "        \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models\",\n",
    "    )\n",
    "    RESULTS_DIR = os.path.join(\n",
    "        \"/content/gdrive/Shared drives/\",\n",
    "        \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/Metrics\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Folder: {}\".format(DATA_DIR))\n",
    "print(os.listdir(DATA_DIR)[:3])\n",
    "print(\"Model Folder: {}\".format(MODEL_DIR))\n",
    "print(os.listdir(MODEL_DIR)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape Govuk Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_soup(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sents_from_soup(soup):\n",
    "    body = soup.findAll(attrs={\"class\": \"gem-c-govspeak\"})\n",
    "    sents = [i.text.split(\"\\n\") for i in body]\n",
    "    sents_clean = [list(filter(None, i)) for i in sents]\n",
    "    return sents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_get_sents(url):\n",
    "    soup = get_page_soup(url)\n",
    "    sents_clean = get_sents_from_soup(soup)\n",
    "    return sents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = url_get_sents(\"https://www.gov.uk/student-visa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Do Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_DIR\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# task = \"ner\"\n",
    "# dataset_name = \"govuk\"\n",
    "# req_date = \"13-12-2021\"\n",
    "# dataset_type = 'FULL'\n",
    "# chkpoint = 'checkpoint-73500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = f\"{MODEL_DIR}/{model_name}-finetuned-{task}-{dataset_name}-{dataset_type}-{req_date}/{chkpoint}\"\n",
    "# OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/content/gdrive/Shareddrives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-14-01-2022-validated_train/checkpoint-1000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Hugging Face Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Huggingface Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = (\n",
    "    \"You must be at least 17 years old to have a drivers licence \"\n",
    "    \"failure to provide this certificate will mean imprisonment in the UK and barring from countries like EU and US\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline with model and tokeniser\n",
    "token_classifier = pipeline(\n",
    "    \"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence)\n",
    "print(len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = token_classifier(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with gov.uk outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_sents = url_get_sents(\n",
    "    url=\"https://www.gov.uk/marriage-visa/documents-you-will-need\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = 0\n",
    "ners = []\n",
    "for i in page_sents[0]:\n",
    "    result = token_classifier(i)\n",
    "    for j in result:\n",
    "        j[\"start\"] += sent_len\n",
    "        j[\"end\"] += sent_len\n",
    "    sent_len += len(i) + 1\n",
    "    ners.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners_flat = [item for sublist in ners for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ents = [(i[\"entity_group\"], i[\"start\"], i[\"end\"]) for i in ners_flat]\n",
    "res_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch sents into one 'doc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \" \".join(page_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"ORG\": \"#7c5cdd\",\n",
    "    \"FORM\": \"#26e21c\",\n",
    "    \"LOC\": \"#eee65c\",\n",
    "    \"MONEY\": \"#80bab2\",\n",
    "    \"SCHEME\": \"#b76d14\",\n",
    "    \"DATE\": \"#bc8251\",\n",
    "    \"STATE\": \"#bd4c33\",\n",
    "    \"PER\": \"#c0970b\",\n",
    "    \"FINANCE\": \"#debdd8\",\n",
    "    \"FORM\": \"#48aba2\",\n",
    "    \"EVENT\": \"#0a8dd9\",\n",
    "    \"CONTACT\": \"#807388\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_entities(text, entities):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for ee in entities:\n",
    "        ents.append(doc.char_span(ee[1], ee[2], ee[0]))\n",
    "    doc.ents = ents\n",
    "    options = {\"distance\": 90, \"colors\": colors}\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_entities(text=seq, entities=res_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ners_and_flatten(sents):\n",
    "    sent_len = 0\n",
    "    ners = []\n",
    "    for i in sents[0]:\n",
    "        result = token_classifier(i)\n",
    "        for j in result:\n",
    "            j[\"start\"] += sent_len\n",
    "            j[\"end\"] += sent_len\n",
    "        sent_len += len(i) + 1\n",
    "        ners.append(result)\n",
    "    ners_flat = [item for sublist in ners for item in sublist]\n",
    "    res_ents = [(i[\"entity_group\"], i[\"start\"], i[\"end\"]) for i in ners_flat]\n",
    "    return res_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_sents(sent_list):\n",
    "    seq = \" \".join(sent_list)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_entities(text, entities):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for ee in entities:\n",
    "        ents.append(doc.char_span(ee[1], ee[2], ee[0]))\n",
    "    doc.ents = ents\n",
    "    options = {\"distance\": 90, \"colors\": colors}\n",
    "    return displacy.render(doc, style=\"ent\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_spacy_viz(url):\n",
    "    print(url)\n",
    "    sents = url_get_sents(url)\n",
    "    flat_ners = get_ners_and_flatten(sents)\n",
    "    stitched = stitch_sents(sents[0])\n",
    "    disp_ents = display_entities(stitched, flat_ners)\n",
    "    return disp_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_to_spacy_viz(\n",
    "    url=\"https://www.gov.uk/hmrc-internal-manuals/tobacco-products-duty/tpd3180\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"/student-visa\",\n",
    "    \"/marriage-visa\",\n",
    "    \"/marriage-visa/eligibility\",\n",
    "    \"/marriage-visa/documents-you-will-need\",\n",
    "    \"/marriage-visa/apply\",\n",
    "    \"/guidance/covid-19-coronavirus-restrictions-what-you-can-and-cannot-do#what-has-changed\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in paths:\n",
    "    url_p = f\"http://www.gov.uk{p}\"\n",
    "    p_dash = p.replace(\"/\", \"_\")\n",
    "    fname = f\"{DATA_DIR}/Images/img_{p_dash}\"\n",
    "    print(fname)\n",
    "    disp_ents = url_to_spacy_viz(url_p)\n",
    "    # output_path = Path(f\"{DATA_DIR}/Images/img_{p_dash}.svg\")\n",
    "    # output_path.open(\"w\", encoding=\"utf-8\").write(disp_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"014_COLAB_ModularTraining","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPHGQIoVlUKbKW+smFI0gr9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Modular Training Notebook\n","\n","This notebook is designed to facilitate fine-tuning of a transformer model on some data."],"metadata":{"id":"r016d93znkQt"}},{"cell_type":"markdown","source":["## 1. Imports and Setup"],"metadata":{"id":"y-kUdXme19k8"}},{"cell_type":"code","source":["# !pip install datasets transformers torch seqeval &> /dev/null"],"metadata":{"id":"wZE4pXf_2ycq","executionInfo":{"status":"ok","timestamp":1640354108748,"user_tz":0,"elapsed":4526,"user":{"displayName":"Rory Hurley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmOUjiNopnhqnZnZgTumlJYolAPylNBseT-IrN=s64","userId":"06667095969102858231"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["import os\n","import datasets\n","import pandas as pd\n","from ast import literal_eval\n","from google.colab import drive"],"metadata":{"id":"hUMjVR36viZ_","executionInfo":{"status":"ok","timestamp":1640348803864,"user_tz":0,"elapsed":7920,"user":{"displayName":"Rory Hurley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmOUjiNopnhqnZnZgTumlJYolAPylNBseT-IrN=s64","userId":"06667095969102858231"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYLGx4HiHsJw","executionInfo":{"status":"ok","timestamp":1640354146310,"user_tz":0,"elapsed":1603,"user":{"displayName":"Rory Hurley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmOUjiNopnhqnZnZgTumlJYolAPylNBseT-IrN=s64","userId":"06667095969102858231"}},"outputId":"8bb76792-4579-4bdb-8fb3-594c510bf547"},"source":["system = \"COLAB\"\n","\n","if system==\"COLAB\":\n","  drive.mount(\"/content/gdrive\")\n","  DATA_DIR = os.path.join(\"/content/gdrive/Shared drives/\", \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data\")\n","  MODEL_DIR = os.path.join(\"/content/gdrive/Shared drives/\", \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models\")\n","  RESULTS_DIR = os.path.join(\"/content/gdrive/Shared drives/\", \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/Metrics\")\n","\n","os.chdir(os.path.join(\"/content/gdrive/Shared drives/\", \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021\"))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## 2. Variables"],"metadata":{"id":"_WzI0unfoM9a"}},{"cell_type":"code","source":["#@title BERT Fine-Tuning Config\n","#@markdown Fill in the below fields to define the environment variables used when training the model.\n","\n","#@markdown Where your model will be saved:\n","ModelFolder = \"/content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models\" #@param {type:\"string\"}\n","#@markdown Path to your pytorch dataset:\n","DataFolder=\"/content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data/samp_hf_govuk_data\" #@param {type:\"string\"}\n","#@markdown Number of GPUs to use:\n","GPUS=1 #@param [\"1\", \"2\", \"3\", \"4\"] {type:\"raw\"}\n","#@markdown Where metrics are saved:\n","OutputDataFolder=\"/content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/Metrics\" #@param {type:\"string\"}\n","\n","\n","%env SM_MODEL_DIR=$ModelFolder\n","%env SM_CHANNEL_DATA=$DataFolder\n","%env SM_NUM_GPUS =$GPUS\n","%env SM_OUTPUT_DATA_DIR=$OutputDataFolder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"4JunEbSWoEEN","executionInfo":{"status":"ok","timestamp":1640356015557,"user_tz":0,"elapsed":192,"user":{"displayName":"Rory Hurley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmOUjiNopnhqnZnZgTumlJYolAPylNBseT-IrN=s64","userId":"06667095969102858231"}},"outputId":"265301b2-7fe4-4b52-f38c-15b6f94150f8"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["env: SM_MODEL_DIR=/content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models\n","env: SM_CHANNEL_DATA=/content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data/samp_hf_govuk_data\n","env: SM_NUM_GPUS=1\n","env: SM_OUTPUT_DATA_DIR=/content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/Metrics\n"]}]},{"cell_type":"markdown","source":["## 3. Training"],"metadata":{"id":"4kdWc22eoGLV"}},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wogUuuFTvPW6","executionInfo":{"status":"ok","timestamp":1640355590896,"user_tz":0,"elapsed":948,"user":{"displayName":"Rory Hurley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmOUjiNopnhqnZnZgTumlJYolAPylNBseT-IrN=s64","userId":"06667095969102858231"}},"outputId":"75ce2a59-356b-4199-dc5c-440f692bd3a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n","\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n","\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n","\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m date\n","\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk, load_metric, ClassLabel, Sequence\n","\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForTokenClassification\n","\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers.trainer_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_last_checkpoint\n","\n","\u001b[34mif\u001b[39;49;00m __name__ == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n","\n","    parser = argparse.ArgumentParser()\n","\n","    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m)\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m)\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m2e-5\u001b[39;49;00m)\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[36mTrue\u001b[39;49;00m)\n","\n","    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n","    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_DATA\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n","    \u001b[37m# parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\u001b[39;49;00m\n","    \u001b[37m# parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\u001b[39;49;00m\n","\n","    args, _ = parser.parse_known_args()\n","\n","    \u001b[37m# Set up logging\u001b[39;49;00m\n","    logger = logging.getLogger(__name__)\n","\n","    logging.basicConfig(\n","        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n","        handlers=[logging.StreamHandler(sys.stdout)],\n","        format=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n","    )\n","\n","    \u001b[37m# load datasets\u001b[39;49;00m\n","    datasets = load_from_disk(args.data_dir)\n","\n","    logger.info(f\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: {len(datasets[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m])}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","    logger.info(f\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: {len(datasets[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m])}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","\n","    label_map = {\u001b[33m\"\u001b[39;49;00m\u001b[33mO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-CONTACT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-DATE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-EVENT\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-FINANCE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-FORM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-LOC\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m6\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-MISC\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m7\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-MONEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m8\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-ORG\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m9\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-PER\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m10\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-SCHEME\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m11\u001b[39;49;00m,\n","             \u001b[33m\"\u001b[39;49;00m\u001b[33mI-STATE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m12\u001b[39;49;00m}\n","    \n","    labels = [\u001b[33m'\u001b[39;49;00m\u001b[33mO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-CONTACT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-DATE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-EVENT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-FINANCE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-FORM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-LOC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-MISC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-MONEY\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-ORG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-PER\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-SCHEME\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mI-STATE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n","\n","    datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].features[f\u001b[33m\"\u001b[39;49;00m\u001b[33mnew_label_list_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = Sequence(feature=ClassLabel(num_classes=\u001b[34m13\u001b[39;49;00m, names=labels, names_file=\u001b[36mNone\u001b[39;49;00m, \u001b[36mid\u001b[39;49;00m=\u001b[36mNone\u001b[39;49;00m), length=-\u001b[34m1\u001b[39;49;00m, \u001b[36mid\u001b[39;49;00m=\u001b[36mNone\u001b[39;49;00m)\n","    datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].features[f\u001b[33m\"\u001b[39;49;00m\u001b[33mnew_label_list_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = Sequence(feature=ClassLabel(num_classes=\u001b[34m13\u001b[39;49;00m, names=labels, names_file=\u001b[36mNone\u001b[39;49;00m, \u001b[36mid\u001b[39;49;00m=\u001b[36mNone\u001b[39;49;00m), length=-\u001b[34m1\u001b[39;49;00m, \u001b[36mid\u001b[39;49;00m=\u001b[36mNone\u001b[39;49;00m)\n","\n","    \u001b[37m#data tokenisation\u001b[39;49;00m\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n","\n","    label_all_tokens = \u001b[36mTrue\u001b[39;49;00m\n","\n","    \u001b[34mdef\u001b[39;49;00m \u001b[32mtokenize_and_align_labels\u001b[39;49;00m(examples):\n","        tokenized_inputs = tokenizer(examples[\u001b[33m\"\u001b[39;49;00m\u001b[33mtext_token\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], truncation=\u001b[36mTrue\u001b[39;49;00m, is_split_into_words=\u001b[36mTrue\u001b[39;49;00m)\n","\n","        labels = []\n","        \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(examples[f\u001b[33m\"\u001b[39;49;00m\u001b[33mnew_label_list_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]):\n","            word_ids = tokenized_inputs.word_ids(batch_index=i) \u001b[37m# Map tokens to their respective word.\u001b[39;49;00m\n","            previous_word_idx = \u001b[36mNone\u001b[39;49;00m\n","            label_ids = []\n","            \u001b[34mfor\u001b[39;49;00m word_idx \u001b[35min\u001b[39;49;00m word_ids:  \u001b[37m# Set the special tokens to -100. Special tokens have a word id that is None. We set the label to -100 so they are automatically ignored in the loss function.\u001b[39;49;00m\n","                \u001b[34mif\u001b[39;49;00m word_idx \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\n","                    label_ids.append(-\u001b[34m100\u001b[39;49;00m)\n","                \u001b[34melif\u001b[39;49;00m word_idx != previous_word_idx: \u001b[37m# Only label the first token of a given word.\u001b[39;49;00m\n","                    label_ids.append(label[word_idx])\n","                \u001b[34melse\u001b[39;49;00m:                   \u001b[37m# For the other tokens in a word, we set the label to either the current label or -100, depending on the label_all_tokens flag.\u001b[39;49;00m\n","                    label_ids.append(label[word_idx] \u001b[34mif\u001b[39;49;00m label_all_tokens \u001b[34melse\u001b[39;49;00m -\u001b[34m100\u001b[39;49;00m)\n","                previous_word_idx = word_idx\n","\n","            labels.append(label_ids)\n","\n","        tokenized_inputs[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = labels\n","        \u001b[34mreturn\u001b[39;49;00m tokenized_inputs\n","\n","    tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=\u001b[36mTrue\u001b[39;49;00m)\n","\n","    \u001b[37m# define metrics\u001b[39;49;00m\n","    metric = load_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33mseqeval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","\n","    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(p):\n","        predictions, labels = p\n","        predictions = np.argmax(predictions, axis=\u001b[34m2\u001b[39;49;00m)\n","\n","        \u001b[37m# Remove ignored index (special tokens)\u001b[39;49;00m\n","        true_predictions = [[label_list[p] \u001b[34mfor\u001b[39;49;00m (p, l) \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(prediction, label) \u001b[34mif\u001b[39;49;00m l != -\u001b[34m100\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m prediction, label \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(predictions, labels)]\n","        true_labels = [[label_list[l] \u001b[34mfor\u001b[39;49;00m (p, l) \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(prediction, label) \u001b[34mif\u001b[39;49;00m l != -\u001b[34m100\u001b[39;49;00m]\u001b[34mfor\u001b[39;49;00m prediction, label \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(predictions, labels)]\n","\n","        results = metric.compute(predictions=true_predictions, references=true_labels)\n","        \n","        \u001b[34mreturn\u001b[39;49;00m {\n","            \u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: results[\u001b[33m\"\u001b[39;49;00m\u001b[33moverall_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n","            \u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: results[\u001b[33m\"\u001b[39;49;00m\u001b[33moverall_recall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n","            \u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: results[\u001b[33m\"\u001b[39;49;00m\u001b[33moverall_f1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n","            \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: results[\u001b[33m\"\u001b[39;49;00m\u001b[33moverall_accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n","        }\n","\n","    \u001b[37m# Prepare model labels - useful in inference API\u001b[39;49;00m\n","    label_list = datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].features[f\u001b[33m\"\u001b[39;49;00m\u001b[33mnew_label_list_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].feature.names\n","    num_labels = \u001b[36mlen\u001b[39;49;00m(label_list)\n","    id2label = {\u001b[36mstr\u001b[39;49;00m(i): label \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(label_list)}\n","    label2id = {v: k \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m id2label.items()}\n","\n","    \u001b[37m# download model from model hub\u001b[39;49;00m\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        args.model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n","    )\n","\n","    \u001b[37m# training\u001b[39;49;00m\n","    model_name = args.model_id.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m]\n","    tod_date = date.today().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","    full_model_name = f\u001b[33m\"\u001b[39;49;00m\u001b[33m{model_name}-finetuned-ner-govuk-{tod_date}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n","    out_path = f\u001b[33m\"\u001b[39;49;00m\u001b[33m{args.output_dir}/{full_model_name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n","\n","    \u001b[37m# data collator\u001b[39;49;00m\n","    data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** collated data *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","\n","    train_args = TrainingArguments(\n","        output_dir=out_path,\n","        evaluation_strategy = \u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n","        learning_rate=\u001b[34m2e-5\u001b[39;49;00m,\n","        per_device_train_batch_size=args.train_batch_size,\n","        per_device_eval_batch_size=args.eval_batch_size,\n","        num_train_epochs=\u001b[34m3\u001b[39;49;00m,\n","        weight_decay=\u001b[34m0.01\u001b[39;49;00m,\n","        push_to_hub=\u001b[36mFalse\u001b[39;49;00m,\n","    )\n","\n","    trainer = Trainer(\n","        model,\n","        train_args,\n","        train_dataset=tokenized_datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n","        eval_dataset=tokenized_datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\n","        data_collator=data_collator,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics\n","    )\n","\n","    trainer.train()\n","\n","    \u001b[37m# get training metrics\u001b[39;49;00m\n","    predictions, labels, _ = trainer.predict(tokenized_datasets[\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n","    predictions = np.argmax(predictions, axis=\u001b[34m2\u001b[39;49;00m)\n","    \u001b[37m# Remove ignored index (special tokens)\u001b[39;49;00m\n","    true_predictions = [\n","        [label_list[p] \u001b[34mfor\u001b[39;49;00m (p, l) \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(prediction, label) \u001b[34mif\u001b[39;49;00m l != -\u001b[34m100\u001b[39;49;00m]\n","        \u001b[34mfor\u001b[39;49;00m prediction, label \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] \u001b[34mfor\u001b[39;49;00m (p, l) \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(prediction, label) \u001b[34mif\u001b[39;49;00m l != -\u001b[34m100\u001b[39;49;00m]\n","        \u001b[34mfor\u001b[39;49;00m prediction, label \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(predictions, labels)\n","    ]\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    results_df = pd.DataFrame(results).T\n","    excel_path = os.path.join(args.output_data_dir, f\u001b[33m\"\u001b[39;49;00m\u001b[33m{full_model_name}_eval_results_overtall.xlsx\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","    results_df.to_excel(excel_path, sheet_name=full_model_name)\n","\n","    \u001b[37m# evaluate model\u001b[39;49;00m\n","    eval_result = trainer.evaluate(eval_dataset=tokenized_datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n","\n","    \u001b[37m# writes eval result to file which can be accessed later\u001b[39;49;00m\n","    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, f\u001b[33m\"\u001b[39;49;00m\u001b[33m{full_model_name}_eval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n","        \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n","            writer.write(f\u001b[33m\"\u001b[39;49;00m\u001b[33m{key} = {value}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","            \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33m{key} = {value}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n","\n","    \u001b[37m# Saves the model to os.environ[\"SM_MODEL_DIR\"] to make sure checkpointing works\u001b[39;49;00m\n","    trainer.save_model(os.path.join(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],full_model_name))\n"]}],"source":["!pygmentize ./Scripts/colab_train.py"]},{"cell_type":"code","source":["!python ./Scripts/colab_train.py --epochs 3 --model_id distilbert-base-uncased --train_batch_size 16 --eval_batch_size 16"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJUUVILMvzrk","executionInfo":{"status":"ok","timestamp":1640355896973,"user_tz":0,"elapsed":304888,"user":{"displayName":"Rory Hurley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmOUjiNopnhqnZnZgTumlJYolAPylNBseT-IrN=s64","userId":"06667095969102858231"}},"outputId":"91eb1d2d-d181-4e64-c103-fc6266eb4a35"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-12-24 14:19:57,227 - __main__ - INFO -  loaded train_dataset length is: 8500\n","2021-12-24 14:19:57,227 - __main__ - INFO -  loaded test_dataset length is: 1500\n","2021-12-24 14:19:58,800 - datasets.arrow_dataset - WARNING - Loading cached processed dataset at /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data/samp_hf_govuk_data/train/cache-5e154d63eac276d1.arrow\n","2021-12-24 14:19:58,840 - datasets.arrow_dataset - WARNING - Loading cached processed dataset at /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Data/samp_hf_govuk_data/test/cache-40b22bfa4cf11242.arrow\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2021-12-24 14:20:01,611 - __main__ - INFO - ***** collated data *****\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: new_label_list_id, text_token.\n","***** Running training *****\n","  Num examples = 8500\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1596\n","{'loss': 0.4513, 'learning_rate': 1.3734335839598997e-05, 'epoch': 0.94}\n"," 31% 500/1596 [01:16<02:41,  6.79it/s]Saving model checkpoint to /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-500\n","Configuration saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-500/config.json\n","Model weights saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-500/special_tokens_map.json\n"," 33% 531/1596 [01:25<02:41,  6.60it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: new_label_list_id, text_token.\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","\n","  0% 0/94 [00:00<?, ?it/s]\u001b[A\n","  4% 4/94 [00:00<00:03, 29.34it/s]\u001b[A\n","  7% 7/94 [00:00<00:03, 24.36it/s]\u001b[A\n"," 11% 10/94 [00:00<00:03, 21.81it/s]\u001b[A\n"," 14% 13/94 [00:00<00:03, 22.45it/s]\u001b[A\n"," 17% 16/94 [00:00<00:03, 21.71it/s]\u001b[A\n"," 20% 19/94 [00:00<00:03, 20.79it/s]\u001b[A\n"," 23% 22/94 [00:01<00:03, 20.65it/s]\u001b[A\n"," 27% 25/94 [00:01<00:03, 19.80it/s]\u001b[A\n"," 30% 28/94 [00:01<00:03, 19.29it/s]\u001b[A\n"," 32% 30/94 [00:01<00:03, 19.38it/s]\u001b[A\n"," 35% 33/94 [00:01<00:03, 19.68it/s]\u001b[A\n"," 38% 36/94 [00:01<00:02, 19.68it/s]\u001b[A\n"," 41% 39/94 [00:01<00:02, 19.82it/s]\u001b[A\n"," 45% 42/94 [00:02<00:02, 20.23it/s]\u001b[A\n"," 48% 45/94 [00:02<00:02, 19.89it/s]\u001b[A\n"," 50% 47/94 [00:02<00:02, 19.68it/s]\u001b[A\n"," 52% 49/94 [00:02<00:02, 19.74it/s]\u001b[A\n"," 54% 51/94 [00:02<00:02, 19.77it/s]\u001b[A\n"," 56% 53/94 [00:02<00:02, 19.72it/s]\u001b[A\n"," 59% 55/94 [00:02<00:02, 18.97it/s]\u001b[A\n"," 61% 57/94 [00:02<00:01, 18.75it/s]\u001b[A\n"," 64% 60/94 [00:02<00:01, 19.49it/s]\u001b[A\n"," 66% 62/94 [00:03<00:01, 19.41it/s]\u001b[A\n"," 69% 65/94 [00:03<00:01, 19.41it/s]\u001b[A\n"," 71% 67/94 [00:03<00:01, 19.15it/s]\u001b[A\n"," 74% 70/94 [00:03<00:01, 19.72it/s]\u001b[A\n"," 78% 73/94 [00:03<00:01, 19.81it/s]\u001b[A\n"," 81% 76/94 [00:03<00:00, 19.99it/s]\u001b[A\n"," 84% 79/94 [00:03<00:00, 20.50it/s]\u001b[A\n"," 87% 82/94 [00:04<00:00, 20.73it/s]\u001b[A\n"," 90% 85/94 [00:04<00:00, 21.42it/s]\u001b[A\n"," 94% 88/94 [00:04<00:00, 22.45it/s]\u001b[A\n"," 97% 91/94 [00:04<00:00, 22.04it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.2574273347854614, 'eval_precision': 0.7048323900740096, 'eval_recall': 0.7224453369031683, 'eval_f1': 0.7135301895107977, 'eval_accuracy': 0.9210285533995899, 'eval_runtime': 5.5788, 'eval_samples_per_second': 268.877, 'eval_steps_per_second': 16.85, 'epoch': 1.0}\n"," 33% 532/1596 [01:31<02:41,  6.60it/s]\n","100% 94/94 [00:05<00:00, 22.72it/s]\u001b[A\n","{'loss': 0.2183, 'learning_rate': 7.468671679197995e-06, 'epoch': 1.88}\n"," 63% 1000/1596 [02:43<01:31,  6.55it/s]Saving model checkpoint to /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1000\n","Configuration saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1000/config.json\n","Model weights saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1000/special_tokens_map.json\n"," 67% 1064/1596 [02:59<01:20,  6.65it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: new_label_list_id, text_token.\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","\n","  0% 0/94 [00:00<?, ?it/s]\u001b[A\n","  4% 4/94 [00:00<00:03, 25.86it/s]\u001b[A\n","  7% 7/94 [00:00<00:03, 22.30it/s]\u001b[A\n"," 11% 10/94 [00:00<00:03, 21.39it/s]\u001b[A\n"," 14% 13/94 [00:00<00:03, 20.99it/s]\u001b[A\n"," 17% 16/94 [00:00<00:03, 20.94it/s]\u001b[A\n"," 20% 19/94 [00:00<00:03, 20.16it/s]\u001b[A\n"," 23% 22/94 [00:01<00:03, 20.41it/s]\u001b[A\n"," 27% 25/94 [00:01<00:03, 20.06it/s]\u001b[A\n"," 30% 28/94 [00:01<00:03, 18.23it/s]\u001b[A\n"," 33% 31/94 [00:01<00:03, 19.06it/s]\u001b[A\n"," 35% 33/94 [00:01<00:03, 18.79it/s]\u001b[A\n"," 38% 36/94 [00:01<00:02, 19.42it/s]\u001b[A\n"," 40% 38/94 [00:01<00:02, 19.33it/s]\u001b[A\n"," 44% 41/94 [00:02<00:02, 19.81it/s]\u001b[A\n"," 46% 43/94 [00:02<00:02, 19.46it/s]\u001b[A\n"," 49% 46/94 [00:02<00:02, 19.91it/s]\u001b[A\n"," 51% 48/94 [00:02<00:02, 19.87it/s]\u001b[A\n"," 53% 50/94 [00:02<00:02, 19.33it/s]\u001b[A\n"," 55% 52/94 [00:02<00:02, 19.19it/s]\u001b[A\n"," 57% 54/94 [00:02<00:02, 18.77it/s]\u001b[A\n"," 60% 56/94 [00:02<00:02, 18.96it/s]\u001b[A\n"," 62% 58/94 [00:02<00:01, 19.15it/s]\u001b[A\n"," 64% 60/94 [00:03<00:01, 19.37it/s]\u001b[A\n"," 67% 63/94 [00:03<00:01, 20.21it/s]\u001b[A\n"," 70% 66/94 [00:03<00:01, 20.80it/s]\u001b[A\n"," 73% 69/94 [00:03<00:01, 21.46it/s]\u001b[A\n"," 77% 72/94 [00:03<00:01, 21.41it/s]\u001b[A\n"," 80% 75/94 [00:03<00:00, 21.95it/s]\u001b[A\n"," 83% 78/94 [00:03<00:00, 21.42it/s]\u001b[A\n"," 86% 81/94 [00:03<00:00, 21.49it/s]\u001b[A\n"," 89% 84/94 [00:04<00:00, 21.64it/s]\u001b[A\n"," 93% 87/94 [00:04<00:00, 22.33it/s]\u001b[A\n"," 96% 90/94 [00:04<00:00, 22.81it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.20745381712913513, 'eval_precision': 0.7545973367152822, 'eval_recall': 0.7965194109772423, 'eval_f1': 0.7749918593292088, 'eval_accuracy': 0.9374033759268023, 'eval_runtime': 5.5208, 'eval_samples_per_second': 271.697, 'eval_steps_per_second': 17.026, 'epoch': 2.0}\n"," 67% 1064/1596 [03:04<01:20,  6.65it/s]\n","100% 94/94 [00:05<00:00, 22.66it/s]\u001b[A\n","{'loss': 0.1683, 'learning_rate': 1.2030075187969925e-06, 'epoch': 2.82}\n"," 94% 1500/1596 [04:11<00:14,  6.47it/s]Saving model checkpoint to /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1500\n","Configuration saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1500/config.json\n","Model weights saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/checkpoint-1500/special_tokens_map.json\n","100% 1595/1596 [04:31<00:00,  6.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: new_label_list_id, text_token.\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","\n","  0% 0/94 [00:00<?, ?it/s]\u001b[A\n","  4% 4/94 [00:00<00:03, 29.57it/s]\u001b[A\n","  7% 7/94 [00:00<00:03, 25.27it/s]\u001b[A\n"," 11% 10/94 [00:00<00:03, 24.27it/s]\u001b[A\n"," 14% 13/94 [00:00<00:03, 23.96it/s]\u001b[A\n"," 17% 16/94 [00:00<00:03, 23.25it/s]\u001b[A\n"," 20% 19/94 [00:00<00:03, 22.28it/s]\u001b[A\n"," 23% 22/94 [00:00<00:03, 22.17it/s]\u001b[A\n"," 27% 25/94 [00:01<00:03, 21.68it/s]\u001b[A\n"," 30% 28/94 [00:01<00:03, 20.60it/s]\u001b[A\n"," 33% 31/94 [00:01<00:02, 21.10it/s]\u001b[A\n"," 36% 34/94 [00:01<00:02, 20.67it/s]\u001b[A\n"," 39% 37/94 [00:01<00:02, 21.01it/s]\u001b[A\n"," 43% 40/94 [00:01<00:02, 20.97it/s]\u001b[A\n"," 46% 43/94 [00:01<00:02, 21.66it/s]\u001b[A\n"," 49% 46/94 [00:02<00:02, 21.68it/s]\u001b[A\n"," 52% 49/94 [00:02<00:02, 21.73it/s]\u001b[A\n"," 55% 52/94 [00:02<00:01, 21.18it/s]\u001b[A\n"," 59% 55/94 [00:02<00:01, 21.01it/s]\u001b[A\n"," 62% 58/94 [00:02<00:01, 21.64it/s]\u001b[A\n"," 65% 61/94 [00:02<00:01, 22.01it/s]\u001b[A\n"," 68% 64/94 [00:02<00:01, 22.45it/s]\u001b[A\n"," 71% 67/94 [00:03<00:01, 22.08it/s]\u001b[A\n"," 74% 70/94 [00:03<00:01, 22.58it/s]\u001b[A\n"," 78% 73/94 [00:03<00:00, 22.60it/s]\u001b[A\n"," 81% 76/94 [00:03<00:00, 22.38it/s]\u001b[A\n"," 84% 79/94 [00:03<00:00, 22.08it/s]\u001b[A\n"," 87% 82/94 [00:03<00:00, 21.71it/s]\u001b[A\n"," 90% 85/94 [00:03<00:00, 22.16it/s]\u001b[A\n"," 94% 88/94 [00:03<00:00, 23.09it/s]\u001b[A\n"," 97% 91/94 [00:04<00:00, 22.90it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.19572320580482483, 'eval_precision': 0.7767838125665602, 'eval_recall': 0.8136992414100848, 'eval_f1': 0.7948131197559115, 'eval_accuracy': 0.9418835778513961, 'eval_runtime': 5.1524, 'eval_samples_per_second': 291.124, 'eval_steps_per_second': 18.244, 'epoch': 3.0}\n","100% 1596/1596 [04:36<00:00,  6.03it/s]\n","100% 94/94 [00:05<00:00, 23.43it/s]\u001b[A\n","                                   \u001b[A\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 277.0134, 'train_samples_per_second': 92.053, 'train_steps_per_second': 5.761, 'train_loss': 0.2723803054121204, 'epoch': 3.0}\n","100% 1596/1596 [04:36<00:00,  5.76it/s]\n","The following columns in the test set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: new_label_list_id, text_token.\n","***** Running Prediction *****\n","  Num examples = 1500\n","  Batch size = 16\n"," 98% 92/94 [00:04<00:00, 22.70it/s]/usr/local/lib/python3.7/dist-packages/openpyxl/workbook/child.py:102: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n","  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: new_label_list_id, text_token.\n","***** Running Evaluation *****\n","  Num examples = 1500\n","  Batch size = 16\n","188it [00:11, 16.12it/s]\n","***** Eval results *****\n","epoch = 3.0\n","\n","eval_accuracy = 0.9418835778513961\n","\n","eval_f1 = 0.7948131197559115\n","\n","eval_loss = 0.19572320580482483\n","\n","eval_precision = 0.7767838125665602\n","\n","eval_recall = 0.8136992414100848\n","\n","eval_runtime = 5.0938\n","\n","eval_samples_per_second = 294.477\n","\n","eval_steps_per_second = 18.454\n","\n","Saving model checkpoint to /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021\n","Configuration saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/config.json\n","Model weights saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/pytorch_model.bin\n","tokenizer config file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/tokenizer_config.json\n","Special tokens file saved in /content/gdrive/Shared drives/GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models/distilbert-base-uncased-finetuned-ner-govuk-24-12-2021/special_tokens_map.json\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"eukearRZ1m4M"},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "from dotenv import load_dotenv # pip install python-dotenv\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# make sure a .env file exists in the same directory, with a line like this:\n",
    "# KG_PWD=<insert password here>\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_PWD = os.environ.get('KG_PWD')\n",
    "KG_PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "def get_results(query, endpoint_url=\"https://query.wikidata.org/sparql\"):\n",
    "    '''\n",
    "    For querying wikidata\n",
    "    '''\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a big list of dictionaries and turn that into a JSON lines file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_PWD=\"nottobeshared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\", user=\"neo4j\", pwd=KG_PWD)\n",
    "\n",
    "q = '''\n",
    "MATCH (n)\n",
    "WHERE n.documentType='form'\n",
    "RETURN n.title, n.description, n.text\n",
    "'''\n",
    "r = conn.query(q)\n",
    "\n",
    "r = pd.DataFrame(r, columns=r[0]._Record__keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_target_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'n.text'\n",
    "\n",
    "subset = r[[field]]\n",
    "subset = subset.rename(columns={field: \"text\"})\n",
    "subset['meta'] = field\n",
    "\n",
    "form_target_df = form_target_df.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_target_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_jsonl(dataframe, sentence_col, meta_cols, outfile):\n",
    "    dict_lines = []\n",
    "    for i, row in tqdm(dataframe.iterrows()):\n",
    "        #dict_line = {\"text\": sentence, \"meta\": {\"base_path\": base_path, \"content_id\": c_id}}\n",
    "        dict_line = {\"text\": row['text'], \"meta\": {i: row[i] for i in meta_cols}}\n",
    "        dict_lines.append(dict_line)\n",
    "    with open(outfile, 'w') as jsonlfile:\n",
    "        jsonlfile.write('\\n'.join(json.dumps(j) for j in dict_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_to_jsonl(dataframe=form_target_df, sentence_col='text', meta_cols=['meta'], outfile='data/interim/targeted_extraction_form.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def has_alpha(inputString):\n",
    "    return any(char.isalpha() for char in inputString)\n",
    "\n",
    "def has_special(inputString):\n",
    "    return any(not char.isalnum() for char in inputString)\n",
    "    \n",
    "def extractFormName(inputString):\n",
    "    inputString = inputString.replace(':', '')\n",
    "    outputString = [token for token in inputString.split() if has_numbers(token) and has_alpha(token) and not has_special(token)]\n",
    "    if outputString:\n",
    "        return outputString[0]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = [{'label': 'FORM', 'pattern': pattern} for pattern in [form for form in r['n.title'].apply(extractFormName) if form]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(forms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting sentences containing forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\", user=\"neo4j\", pwd=KG_PWD)\n",
    "\n",
    "forms_list = [x['pattern'] for x in forms]\n",
    "\n",
    "q = '''\n",
    "WITH {items} AS items\n",
    "MATCH (n)\n",
    "// This bit splits up a text into a list of tokens, then determines if any forms\n",
    "// are in that list\n",
    "WHERE any(item IN items WHERE item IN split(n.text, ' '))\n",
    "RETURN n.name, n.text\n",
    "LIMIT 100\n",
    "'''.format(items=forms_list)\n",
    "\n",
    "r = conn.query(q)\n",
    "\n",
    "p = pd.DataFrame(r, columns=r[0]._Record__keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = []\n",
    "\n",
    "# iterate over each page and look at its text\n",
    "for text in p['n.text']:\n",
    "    # extract the sentences from the page with a crude heuristic, then iterate over those\n",
    "    for sentence in text.split('. '):\n",
    "        # extract the tokens from the sentence\n",
    "        # this avoids partial matches\n",
    "        split_sentence = sentence.split()\n",
    "        # look at each form and check if its in the sentence\n",
    "        for form in forms_list:\n",
    "            # add example of form in use\n",
    "            if form in split_sentence:\n",
    "                form_examples.append({'form': form, 'sentence': sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = pd.DataFrame(form_examples)\n",
    "form_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = form_examples.drop_duplicates(subset=['sentence'])\n",
    "form_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERSON NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### govGraph names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\", user=\"neo4j\", pwd=KG_PWD)\n",
    "\n",
    "q = '''\n",
    "MATCH (n)\n",
    "WHERE n.documentType='person'\n",
    "RETURN n.title\n",
    "'''\n",
    "r = conn.query(q)\n",
    "\n",
    "person_name = [{'label': 'PERSON (NAME)', 'pattern': pattern} for pattern in pd.DataFrame(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(person_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikiData names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10000 examples of everything that is an 'instance of' 'human'\n",
    "\n",
    "query = '''SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q5.\n",
    "    }\n",
    "    LIMIT 10000\n",
    "  }\n",
    "}'''\n",
    "\n",
    "names = get_results(query)\n",
    "names = pd.json_normalize(names['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Q numbers and very long names\n",
    "\n",
    "person_names_wiki = [{'label': 'PERSON (NAME)', 'pattern': pattern} for pattern in [name for name in names if not name[-1].isdigit() and len(name.split()) < 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(person_names_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is an 'instance of' 'job'\n",
    "\n",
    "query = '''SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q192581.\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "jobs = get_results(query)\n",
    "jobs = pd.json_normalize(jobs['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Q numbers and very long job titles\n",
    "jobs = [job for job in jobs if not job[-1].isdigit() and len(job.split()) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is an 'instance of' 'profession'\n",
    "\n",
    "query = '''SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q28640.\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "professions = get_results(query)\n",
    "professions = pd.json_normalize(professions['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Q numbers and very long job titles\n",
    "professions = [profession for profession in professions if not profession[-1].isdigit() and len(profession.split()) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique jobs and professions\n",
    "unique_jobs = [{'label': 'PERSON (PROFESSION)', 'pattern': pattern} for pattern in list(set(jobs) | set(professions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(unique_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERSON ROLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ministerial roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\", user=\"neo4j\", pwd=KG_PWD)\n",
    "\n",
    "q = '''\n",
    "MATCH (n)\n",
    "WHERE n.documentType='ministerial_role'\n",
    "RETURN n.title\n",
    "'''\n",
    "r = conn.query(q)\n",
    "\n",
    "ministerial_roles = [{'label': 'PERSON (ROLE)','pattern': pattern} for pattern in pd.DataFrame(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(ministerial_roles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORGANISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\", user=\"neo4j\", pwd=KG_PWD)\n",
    "\n",
    "q = '''\n",
    "MATCH (n)\n",
    "WHERE n.documentType='organisation'\n",
    "RETURN n.name, n.title\n",
    "'''\n",
    "r = conn.query(q)\n",
    "\n",
    "public_organisations = pd.DataFrame(r, columns=r[0]._Record__keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_acronyms = []\n",
    "\n",
    "for (_, row) in tqdm(public_organisations.iterrows()):\n",
    "    try:\n",
    "        pass\n",
    "        response = requests.get(f'https://www.gov.uk/api/search.json?filter_link=' + row['n.name'])\n",
    "        org_acronyms.append(response.json()['results'][0]['organisations'][0]['acronym'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_organisations = [{'label': 'ORGANISATION (PUBLIC)', 'pattern': pattern} for pattern in public_organisations['n.title']]\n",
    "\n",
    "org_acronyms = [{'label': 'ORGANISATION (PUBLIC)', 'pattern': pattern} for pattern in org_acronyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(public_organisations)\n",
    "json_file.extend(org_acronyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Departments of the UK Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything than is an 'instance of' 'department of the United Kingdom Government'\n",
    "\n",
    "query = '''SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q2500378.\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "gov_dept = get_results(query)\n",
    "gov_dept = pd.json_normalize(gov_dept['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_dept = [{'label': 'ORGANISATION (PUBLIC)', 'pattern': pattern} for pattern in gov_dept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(gov_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nasdaq-100 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is 'part of' 'Nasdaq-100'\n",
    "\n",
    "query = '''SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P361 ?statement0.\n",
    "      ?statement0 (ps:P361/(wdt:P279*)) wd:Q507306.\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "nasdaq_100 = get_results(query)\n",
    "nasdaq_100 = pd.json_normalize(nasdaq_100['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_100 = [{'label': 'ORGANISATION (PRIVATE)', 'pattern': pattern} for pattern in nasdaq_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(nasdaq_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTSE-100 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is 'part of' 'FTSE 100 Index'\n",
    "\n",
    "query = '''SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P361 ?statement0.\n",
    "      ?statement0 (ps:P361/(wdt:P279*)) wd:Q466496.\n",
    "    }\n",
    "  }\n",
    "}'''\n",
    "\n",
    "ftse_100 = get_results(query)\n",
    "ftse_100 = pd.json_normalize(ftse_100['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftse_100 = [{'label': 'ORGANISATION (PRIVATE)', 'pattern': pattern} for pattern in ftse_100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(ftse_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Life event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site= \"https://simplicable.com/en/life-events\"\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = Request(site,headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page)\n",
    "life_events = [td.find('span', class_='blogy').text for td in soup.findAll('td', class_='tdFlatList')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_events = [{'label': 'EVENT (LIFE EVENT)', 'pattern': pattern} for pattern in life_events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(life_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCHEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = []\n",
    "for page_num in range(1,5):\n",
    "    page = requests.get(f'https://www.gov.uk/business-finance-support?page={page_num}')\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    for scheme in soup.find_all('a', class_='gem-c-document-list__item-title'):\n",
    "        schemes.append(scheme.text)\n",
    "\n",
    "schemes = [scheme.split('-')[0].strip() for scheme in schemes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = [{'label': 'SCHEME', 'pattern': pattern} for pattern in schemes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(schemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOCATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries and capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get countries and capitals\n",
    "\n",
    "query = \"\"\"#List of present-day countries and capital(s)\n",
    "SELECT DISTINCT ?country ?countryLabel ?capital ?capitalLabel\n",
    "WHERE\n",
    "{\n",
    "  ?country wdt:P31 wd:Q3624078 .\n",
    "  #not a former country\n",
    "  FILTER NOT EXISTS {?country wdt:P31 wd:Q3024240}\n",
    "  #and no an ancient civilisation (needed to exclude ancient Egypt)\n",
    "  FILTER NOT EXISTS {?country wdt:P31 wd:Q28171280}\n",
    "  OPTIONAL { ?country wdt:P36 ?capital } .\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\n",
    "ORDER BY ?countryLabel\"\"\"\n",
    "\n",
    "countries_capitals = get_results(query)\n",
    "countries_capitals = pd.json_normalize(countries_capitals['results']['bindings'])[['countryLabel.value', 'capitalLabel.value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [{'label': 'LOCATION (GPE)', 'pattern': pattern} for pattern in countries_capitals['countryLabel.value']]\n",
    "capitals = [{'label': 'LOCATION (GPE)', 'pattern': pattern} for pattern in countries_capitals['capitalLabel.value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(countries)\n",
    "json_file.extend(capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United Kingdom Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything with 'country' 'United Kingdom' AND 'instance of' 'county'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P17 ?statement0.\n",
    "      ?statement0 (ps:P17/(wdt:P279*)) wd:Q145.\n",
    "      ?item p:P31 ?statement1.\n",
    "      ?statement1 (ps:P31/(wdt:P279*)) wd:Q28575.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "counties = get_results(query)\n",
    "counties = pd.json_normalize(counties['results']['bindings'])['itemLabel.value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = [{'label': 'GPE', 'pattern': pattern} for pattern in counties]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split multi word entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_json_file = [{'label': file['label'], 'pattern': [{'lower': word.lower()} for word in str(file['pattern']).split()]} for file in json_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to JSON lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('patterns.jsonl', 'w') as outfile:\n",
    "    for entry in split_json_file:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb78078aff4f0fca0b87149217d885ad4dd99935d8eaa10d490c5420d431fcbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

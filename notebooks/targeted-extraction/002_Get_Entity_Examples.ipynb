{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv  # pip install python-dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from SPARQLWrapper import JSON, SPARQLWrapper\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 400)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# make sure a .env file exists in the same directory, with a line like this:\n",
    "# KG_PWD=<insert password here>\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_PWD = os.environ.get(\"KG_PWD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(\n",
    "                self.__uri, auth=(self.__user, self.__pwd)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "\n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "\n",
    "    def query(self, query, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = (\n",
    "                self.__driver.session(database=db)\n",
    "                if db is not None\n",
    "                else self.__driver.session()\n",
    "            )\n",
    "            response = list(session.run(query))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "\n",
    "def get_results(query, endpoint_url=\"https://query.wikidata.org/sparql\"):\n",
    "    \"\"\"\n",
    "    For querying wikidata\n",
    "    \"\"\"\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (\n",
    "        sys.version_info[0],\n",
    "        sys.version_info[1],\n",
    "    )\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to make a big list of dictionaries and turn that into a JSON lines file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_PWD = \"nottobeshared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(\n",
    "    uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "    user=\"neo4j\",\n",
    "    pwd=KG_PWD,\n",
    ")\n",
    "\n",
    "q = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE n.documentType='form'\n",
    "RETURN n.title, n.description, n.text\n",
    "\"\"\"\n",
    "r = conn.query(q)\n",
    "\n",
    "r = pd.DataFrame(r, columns=r[0]._Record__keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_target_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = \"n.text\"\n",
    "\n",
    "subset = r[[field]]\n",
    "subset = subset.rename(columns={field: \"text\"})\n",
    "subset[\"meta\"] = field\n",
    "\n",
    "form_target_df = form_target_df.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_target_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_jsonl(dataframe, sentence_col, meta_cols, outfile):\n",
    "    dict_lines = []\n",
    "    for i, row in tqdm(dataframe.iterrows()):\n",
    "        # dict_line = {\"text\": sentence, \"meta\": {\"base_path\": base_path, \"content_id\": c_id}}\n",
    "        dict_line = {\"text\": row[\"text\"], \"meta\": {i: row[i] for i in meta_cols}}\n",
    "        dict_lines.append(dict_line)\n",
    "    with open(outfile, \"w\") as jsonlfile:\n",
    "        jsonlfile.write(\"\\n\".join(json.dumps(j) for j in dict_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_to_jsonl(\n",
    "    dataframe=form_target_df,\n",
    "    sentence_col=\"text\",\n",
    "    meta_cols=[\"meta\"],\n",
    "    outfile=\"data/interim/targeted_extraction_form.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "\n",
    "def has_alpha(inputString):\n",
    "    return any(char.isalpha() for char in inputString)\n",
    "\n",
    "\n",
    "def has_special(inputString):\n",
    "    return any(not char.isalnum() for char in inputString)\n",
    "\n",
    "\n",
    "def extractFormName(inputString):\n",
    "    inputString = inputString.replace(\":\", \"\")\n",
    "    outputString = [\n",
    "        token\n",
    "        for token in inputString.split()\n",
    "        if has_numbers(token) and has_alpha(token) and not has_special(token)\n",
    "    ]\n",
    "    if outputString:\n",
    "        return outputString[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = [\n",
    "    {\"label\": \"FORM\", \"pattern\": pattern}\n",
    "    for pattern in [form for form in r[\"n.title\"].apply(extractFormName) if form]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(forms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting sentences containing forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(\n",
    "    uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "    user=\"neo4j\",\n",
    "    pwd=KG_PWD,\n",
    ")\n",
    "\n",
    "forms_list = [x[\"pattern\"] for x in forms]\n",
    "\n",
    "q = \"\"\"\n",
    "WITH {items} AS items\n",
    "MATCH (n)\n",
    "// This bit splits up a text into a list of tokens, then determines if any forms\n",
    "// are in that list\n",
    "WHERE any(item IN items WHERE item IN split(n.text, ' '))\n",
    "RETURN n.name, n.text\n",
    "LIMIT 500\n",
    "\"\"\".format(\n",
    "    items=forms_list\n",
    ")\n",
    "\n",
    "r = conn.query(q)\n",
    "\n",
    "p = pd.DataFrame(r, columns=r[0]._Record__keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = []\n",
    "\n",
    "# iterate over each page and look at its text\n",
    "for text in p[\"n.text\"]:\n",
    "    # extract the sentences from the page with a crude heuristic, then iterate over those\n",
    "    for sentence in text.split(\". \"):\n",
    "        # extract the tokens from the sentence\n",
    "        # this avoids partial matches\n",
    "        split_sentence = sentence.split()\n",
    "        # look at each form and check if its in the sentence\n",
    "        for form in forms_list:\n",
    "            # add example of form in use\n",
    "            if form in split_sentence:\n",
    "                form_examples.append({\"form\": form, \"sentence\": sentence})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = pd.DataFrame(form_examples)\n",
    "form_examples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = form_examples.drop_duplicates(subset=[\"sentence\"])\n",
    "form_examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_examples = form_examples.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get into prodigy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_jsonl(dataframe, sentence_col, outfile):\n",
    "    \"\"\"\n",
    "    Convert a column containing list of sentences for each row into .jsonl file format for Prodigy. Specify metacols and file path to save .jsonl to.\n",
    "    \"\"\"\n",
    "    dict_lines = []\n",
    "    for i, row in tqdm(dataframe.head().iterrows()):\n",
    "        for sentence in row[sentence_col]:\n",
    "            # dict_line = {\"text\": sentence, \"meta\": {\"base_path\": base_path, \"content_id\": c_id}}\n",
    "            dict_line = {\"text\": sentence, \"meta\": {\"source\": \"TE Notebook\"}}\n",
    "            dict_lines.append(dict_line)\n",
    "    with open(outfile, \"w\") as jsonlfile:\n",
    "        jsonlfile.write(\"\\n\".join(json.dumps(j) for j in dict_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_jsonl(dataframe, sentence_col, outfile):\n",
    "    \"\"\"\n",
    "    Convert a column containing list of sentences for each row into .jsonl file format for Prodigy. Specify metacols and file path to save .jsonl to.\n",
    "    \"\"\"\n",
    "    dict_lines = []\n",
    "    for sentence in dataframe[sentence_col]:\n",
    "        # dict_line = {\"text\": sentence, \"meta\": {\"base_path\": base_path, \"content_id\": c_id}}\n",
    "        dict_line = {\"text\": sentence, \"meta\": {\"source\": \"TE Notebook\"}}\n",
    "        dict_lines.append(dict_line)\n",
    "    with open(outfile, \"w\") as jsonlfile:\n",
    "        jsonlfile.write(\"\\n\".join(json.dumps(j) for j in dict_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_to_jsonl(\n",
    "    form_examples,\n",
    "    \"sentence\",\n",
    "    \"/Users/roryhurley/Documents/GitHub/govuk-content-metadata/data/interim/targeted_extraction_forms_100_falsepositives.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERSON NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### govGraph names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(\n",
    "    uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "    user=\"neo4j\",\n",
    "    pwd=KG_PWD,\n",
    ")\n",
    "\n",
    "q = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE n.documentType='person'\n",
    "RETURN n.title\n",
    "\"\"\"\n",
    "r = conn.query(q)\n",
    "\n",
    "person_name = [\n",
    "    {\"label\": \"PERSON (NAME)\", \"pattern\": pattern} for pattern in pd.DataFrame(r)[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(person_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikiData names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 10000 examples of everything that is an 'instance of' 'human'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q5.\n",
    "    }\n",
    "    LIMIT 10000\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "names = get_results(query)\n",
    "names = pd.json_normalize(names[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Q numbers and very long names\n",
    "\n",
    "person_names_wiki = [\n",
    "    {\"label\": \"PERSON (NAME)\", \"pattern\": pattern}\n",
    "    for pattern in [\n",
    "        name for name in names if not name[-1].isdigit() and len(name.split()) < 4\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(person_names_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is an 'instance of' 'job'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q192581.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "jobs = get_results(query)\n",
    "jobs = pd.json_normalize(jobs[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Q numbers and very long job titles\n",
    "jobs = [job for job in jobs if not job[-1].isdigit() and len(job.split()) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is an 'instance of' 'profession'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q28640.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "professions = get_results(query)\n",
    "professions = pd.json_normalize(professions[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Q numbers and very long job titles\n",
    "professions = [\n",
    "    profession\n",
    "    for profession in professions\n",
    "    if not profession[-1].isdigit() and len(profession.split()) < 4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique jobs and professions\n",
    "unique_jobs = [\n",
    "    {\"label\": \"PERSON (PROFESSION)\", \"pattern\": pattern}\n",
    "    for pattern in list(set(jobs) | set(professions))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(unique_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERSON ROLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ministerial roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(\n",
    "    uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "    user=\"neo4j\",\n",
    "    pwd=KG_PWD,\n",
    ")\n",
    "\n",
    "q = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE n.documentType='ministerial_role'\n",
    "RETURN n.title\n",
    "\"\"\"\n",
    "r = conn.query(q)\n",
    "\n",
    "ministerial_roles = [\n",
    "    {\"label\": \"PERSON (ROLE)\", \"pattern\": pattern} for pattern in pd.DataFrame(r)[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(ministerial_roles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORGANISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Neo4jConnection(\n",
    "    uri=\"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "    user=\"neo4j\",\n",
    "    pwd=KG_PWD,\n",
    ")\n",
    "\n",
    "q = \"\"\"\n",
    "MATCH (n)\n",
    "WHERE n.documentType='organisation'\n",
    "RETURN n.name, n.title\n",
    "\"\"\"\n",
    "r = conn.query(q)\n",
    "\n",
    "public_organisations = pd.DataFrame(r, columns=r[0]._Record__keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_acronyms = []\n",
    "\n",
    "for (_, row) in tqdm(public_organisations.iterrows()):\n",
    "    try:\n",
    "        pass\n",
    "        response = requests.get(\n",
    "            f\"https://www.gov.uk/api/search.json?filter_link=\" + row[\"n.name\"]\n",
    "        )\n",
    "        org_acronyms.append(\n",
    "            response.json()[\"results\"][0][\"organisations\"][0][\"acronym\"]\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_organisations = [\n",
    "    {\"label\": \"ORGANISATION (PUBLIC)\", \"pattern\": pattern}\n",
    "    for pattern in public_organisations[\"n.title\"]\n",
    "]\n",
    "\n",
    "org_acronyms = [\n",
    "    {\"label\": \"ORGANISATION (PUBLIC)\", \"pattern\": pattern} for pattern in org_acronyms\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(public_organisations)\n",
    "json_file.extend(org_acronyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Departments of the UK Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything than is an 'instance of' 'department of the United Kingdom Government'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P31 ?statement0.\n",
    "      ?statement0 (ps:P31/(wdt:P279*)) wd:Q2500378.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "gov_dept = get_results(query)\n",
    "gov_dept = pd.json_normalize(gov_dept[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_dept = [\n",
    "    {\"label\": \"ORGANISATION (PUBLIC)\", \"pattern\": pattern} for pattern in gov_dept\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(gov_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nasdaq-100 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is 'part of' 'Nasdaq-100'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P361 ?statement0.\n",
    "      ?statement0 (ps:P361/(wdt:P279*)) wd:Q507306.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "nasdaq_100 = get_results(query)\n",
    "nasdaq_100 = pd.json_normalize(nasdaq_100[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_100 = [\n",
    "    {\"label\": \"ORGANISATION (PRIVATE)\", \"pattern\": pattern} for pattern in nasdaq_100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(nasdaq_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTSE-100 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything that is 'part of' 'FTSE 100 Index'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P361 ?statement0.\n",
    "      ?statement0 (ps:P361/(wdt:P279*)) wd:Q466496.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "ftse_100 = get_results(query)\n",
    "ftse_100 = pd.json_normalize(ftse_100[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftse_100 = [\n",
    "    {\"label\": \"ORGANISATION (PRIVATE)\", \"pattern\": pattern} for pattern in ftse_100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(ftse_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Life event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = \"https://simplicable.com/en/life-events\"\n",
    "hdr = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "req = Request(site, headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page)\n",
    "life_events = [\n",
    "    td.find(\"span\", class_=\"blogy\").text\n",
    "    for td in soup.findAll(\"td\", class_=\"tdFlatList\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_events = [\n",
    "    {\"label\": \"EVENT (LIFE EVENT)\", \"pattern\": pattern} for pattern in life_events\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(life_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCHEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = []\n",
    "for page_num in range(1, 5):\n",
    "    page = requests.get(f\"https://www.gov.uk/business-finance-support?page={page_num}\")\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    for scheme in soup.find_all(\"a\", class_=\"gem-c-document-list__item-title\"):\n",
    "        schemes.append(scheme.text)\n",
    "\n",
    "schemes = [scheme.split(\"-\")[0].strip() for scheme in schemes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = [{\"label\": \"SCHEME\", \"pattern\": pattern} for pattern in schemes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(schemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOCATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries and capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get countries and capitals\n",
    "\n",
    "query = \"\"\"#List of present-day countries and capital(s)\n",
    "SELECT DISTINCT ?country ?countryLabel ?capital ?capitalLabel\n",
    "WHERE\n",
    "{\n",
    "  ?country wdt:P31 wd:Q3624078 .\n",
    "  #not a former country\n",
    "  FILTER NOT EXISTS {?country wdt:P31 wd:Q3024240}\n",
    "  #and no an ancient civilisation (needed to exclude ancient Egypt)\n",
    "  FILTER NOT EXISTS {?country wdt:P31 wd:Q28171280}\n",
    "  OPTIONAL { ?country wdt:P36 ?capital } .\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\n",
    "ORDER BY ?countryLabel\"\"\"\n",
    "\n",
    "countries_capitals = get_results(query)\n",
    "countries_capitals = pd.json_normalize(countries_capitals[\"results\"][\"bindings\"])[\n",
    "    [\"countryLabel.value\", \"capitalLabel.value\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\n",
    "    {\"label\": \"LOCATION (GPE)\", \"pattern\": pattern}\n",
    "    for pattern in countries_capitals[\"countryLabel.value\"]\n",
    "]\n",
    "capitals = [\n",
    "    {\"label\": \"LOCATION (GPE)\", \"pattern\": pattern}\n",
    "    for pattern in countries_capitals[\"capitalLabel.value\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(countries)\n",
    "json_file.extend(capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United Kingdom Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything with 'country' 'United Kingdom' AND 'instance of' 'county'\n",
    "\n",
    "query = \"\"\"SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    SELECT DISTINCT ?item WHERE {\n",
    "      ?item p:P17 ?statement0.\n",
    "      ?statement0 (ps:P17/(wdt:P279*)) wd:Q145.\n",
    "      ?item p:P31 ?statement1.\n",
    "      ?statement1 (ps:P31/(wdt:P279*)) wd:Q28575.\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "counties = get_results(query)\n",
    "counties = pd.json_normalize(counties[\"results\"][\"bindings\"])[\"itemLabel.value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = [{\"label\": \"GPE\", \"pattern\": pattern} for pattern in counties]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file.extend(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split multi word entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_json_file = [\n",
    "    {\n",
    "        \"label\": file[\"label\"],\n",
    "        \"pattern\": [{\"lower\": word.lower()} for word in str(file[\"pattern\"]).split()],\n",
    "    }\n",
    "    for file in json_file\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to JSON lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"patterns.jsonl\", \"w\") as outfile:\n",
    "    for entry in split_json_file:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb78078aff4f0fca0b87149217d885ad4dd99935d8eaa10d490c5420d431fcbd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

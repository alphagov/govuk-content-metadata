main:
  steps:
    - init:
        assign:
          - projectid: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: ${sys.get_env("GOOGLE_CLOUD_LOCATION")}
          - bucket: cpto-content-metadata
          - gs_folder_configs: vertex_ner_daily_batchjob
          - finalFileName: "named_entities_counts/named_entities_counts.csv.gz"
          - prefix: "named_entities_counts/temp_csvs"
          - listResultDelete:
              nextPageToken: ""
          - listResult:
              nextPageToken: ""
          # CHANGE DATASET NAME BELOW (both queries!) after testing
          - query_update_entity_tables: update_entity_accumulation.txt
          - query_headers: "SELECT * FROM `cpto-content-metadata.named_entities_COPY.named_entities_counts` LIMIT 0"
          # CHANGE dataset id after testing
          - bq_dataset_id: named_entities_COPY
          - bq_table_to_export: "named_entities_counts"
          - create_disposition: "CREATE_IF_NEEDED"  # create a new one if table doesn't exist
          - write_disposition: "WRITE_TRUNCATE"  # truncate it if the table already exists
    - read_update_table_query_from_gcs:
        call: googleapis.storage.v1.objects.get
        args:
            bucket: ${bucket}
            object: ${gs_folder_configs + "%2F" + query_update_entity_tables}
            alt: "media"
        result: update_tables_query
    - run-update-entity-tables-query:
            call: googleapis.bigquery.v2.jobs.query
            args:
              projectId: ${projectid}
              body:
                query: ${update_tables_query}
                useLegacySql: false
    - deleteExisitngTempCsv:
            call: list_and_delete_files
            args:
              pagetoken: ${listResultDelete.nextPageToken}
              bucket: ${bucket}
              prefix: ${prefix}
              projectid: ${projectid}
    - waitForGCS1: # fix latency issue with Cloud Storage
            call: sys.sleep
            args:
              seconds: 1
    - create-file:
            call: http.post
            args:
                url: ${"https://storage.googleapis.com/upload/storage/v1/b/" + bucket + "/o?name=" + finalFileName + "&uploadType=media"}
                auth:
                    type: OAuth2
    - get-headers-bq:
            call: googleapis.bigquery.v2.jobs.query
            args:
              projectId: ${projectid}
              body:
                query: ${"EXPORT DATA OPTIONS( uri='gs://" + bucket + "/" + prefix + "/" + "*.csv.gz', format='CSV', compression='GZIP', overwrite=true, header=true) AS " + query_headers}
                useLegacySql: false
    - export-bq-table-to-gcs:
        call: googleapis.bigquery.v2.jobs.insert
        args:
            projectId: ${projectid}
            body:
                configuration:
                    extract:
                        compression: "GZIP"
                        destinationFormat: "CSV"
                        destinationUris: ['${"gs://" + bucket + "/" + prefix + "/" + "/z*.csv.gz"}']
                        fieldDelimiter: ","
                        printHeader: false
                        sourceTable:
                            projectId: ${projectid}
                            datasetId: ${bq_dataset_id}
                            tableId: ${bq_table_to_export}
    - composefiles:
                call: list_and_compose_file
                args:
                  pagetoken: ${listResult.nextPageToken}
                  bucket: ${bucket}
                  prefix: ${prefix}
                  projectid: ${projectid}
                  finalFileName: ${finalFileName}
                result: listResult
    - missing-files: # Non recursive loop, to prevent depth errors
            switch:
              - condition: ${"nextPageToken" in listResult}
                next: composefiles
    - the_end:
        return: "SUCCESS"

list_and_delete_files:
    params:
      - pagetoken
      - bucket
      - prefix
      - projectid
    steps:
      - list-files:
          call: googleapis.storage.v1.objects.list
          args:
            bucket: ${bucket}
            pageToken: ${pagetoken}
            prefix: ${prefix}
            maxResults: 1000
          result: listResultDelete
      - delete-files:
          try:
              for:
                value: file
                in: ${listResultDelete.items}
                steps:
                    - delete-file:
                          call: googleapis.storage.v1.objects.delete
                          args:
                            bucket: ${bucket}
                            object: ${text.url_encode(file.name)}
                            userProject: ${projectid}
                          result: deleteResult
          except:
            as: e
            steps:
              - known_errors:
                  switch:
                    - condition: ${not("KeyError" in e)}
                      return: "files do not exists"

# source: https://medium.com/google-cloud/get-a-single-one-csv-file-with-bigquery-export-956d2a147886
list_and_compose_file:
    params:
      - pagetoken
      - bucket
      - prefix
      - projectid
      - finalFileName
    steps:
      - list-files:
          call: googleapis.storage.v1.objects.list
          args:
            bucket: ${bucket}
            pageToken: ${pagetoken}
            prefix: ${prefix}
            maxResults: 62
          result: listResult
      - init-iter:
          assign:
            - finalFileFormatted:
                name: ${finalFileName}
            - fileList:
                - ${finalFileFormatted}
      - process-files:
          for:
            value: file
            in: ${listResult.items}
            steps:
              - concat-file:
                  assign:
                    - fileFormatted:
                        name: ${file.name}
                    - fileList: ${list.concat(fileList, fileFormatted)}
              - test-concat:
                  switch:
                    - condition: ${len(fileList) == 32}
                      steps:
                        - compose-files:
                            call: compose_file
                            args:
                              fileList: ${fileList}
                              projectid: ${projectid}
                              bucket: ${bucket}
                              finalFileName: ${finalFileName}
                            next: init-for-iter
                        - init-for-iter:
                            assign:
                              - fileList:
                                  - ${finalFileFormatted}
      - finish-compose: # Process the latest files in the fileList buffer
          switch:
            - condition: ${len(fileList) > 1} # If there is more than the finalFileName in the list
              steps:
                - last-compose-files:
                    call: compose_file
                    args:
                      fileList: ${fileList}
                      projectid: ${projectid}
                      bucket: ${bucket}
                      finalFileName: ${finalFileName}
      - return-step:
          return: ${listResult}

compose_file:
  params:
    - fileList
    - projectid
    - bucket
    - finalFileName
  steps:
    - compose:
        call: googleapis.storage.v1.objects.compose
        args:
          destinationBucket: ${bucket}
          destinationObject: ${text.url_encode(finalFileName)}
          body:
            sourceObjects: ${fileList}
